{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相似度筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,QuantileTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "df = pd.read_csv(r\"F:\\cache_data\\frequency_filter\\gl\\soil_type_point.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.select_dtypes(include='number').mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 区分点位\n",
    "no_calc_df = df[df['label']=='inner']\n",
    "train_df = df[df['label']=='fish_net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['analyticalhillshading', 'aspect',\n",
    "       'channelnetworkbaselevel', 'channelnetworkdistance', \n",
    "       'convergenceindex',  'dem', 'dissimilarity', 'dl', 'dz',\n",
    "       'entropy', 'etp2022', 'etp20221', 'etp202210', 'etp202211', 'etp202212',\n",
    "       'etp20222', 'etp20223', 'etp20224', 'etp20225', 'etp20226', 'etp20227',\n",
    "       'etp20228', 'etp20229', 'etp2022mean', 'evi', 'gl_slope_101',\n",
    "       'lat', 'lon', 'lsfactor', 'lswi', 'mean', 'mndwi',\n",
    "       'mrrtf', 'mrvbf', 'ndmi', 'ndvi', 'ndwi', 'night2022', 'pca1', 'pca2',\n",
    "       'plancurvature', 'pre2022', 'pre20221', 'pre202210', 'pre202211',\n",
    "       'pre202212', 'pre20222', 'pre20223', 'pre20224', 'pre20225', 'pre20226',\n",
    "       'pre20227', 'pre20228', 'pre20229', 'pre2022mean', 'profilecurvature',\n",
    "       'relativeslopeposition', 'savi',  'slope', 'tmp2022',\n",
    "       'tmp20221', 'tmp202210', 'tmp202211', 'tmp202212', 'tmp20222',\n",
    "       'tmp20223', 'tmp20224', 'tmp20225', 'tmp20226', 'tmp20227', 'tmp20228',\n",
    "       'tmp20229', 'tmp2022mean', 'topographicwetnessindex',\n",
    "       'totalcatchmentarea', 'valleydepth', 'vari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签值\n",
    "label_column = 'TZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取标签的唯一值\n",
    "label_values = train_df[label_column].unique()\n",
    "\n",
    "# 初始化用于存储代表性样本的列表\n",
    "representative_samples = []\n",
    "\n",
    "# 设置相似度阈值\n",
    "threshold = 0.80  # 根据需要调整\n",
    "\n",
    "# 遍历每个标签值\n",
    "for label_value in label_values:\n",
    "    # 提取当前标签值的样本\n",
    "    label_df = train_df[train_df[label_column] == label_value]\n",
    "    \n",
    "    # 检查样本数量\n",
    "    num_samples = len(label_df)\n",
    "    if num_samples == 1:\n",
    "        # 如果样本数量为1，直接添加该样本\n",
    "        representative_samples.append(label_df.iloc[0].values)\n",
    "    else:\n",
    "        # 对特征进行标准化\n",
    "        scaler = StandardScaler()\n",
    "        # scaler = MinMaxScaler()\n",
    "        # scaler = MaxAbsScaler()\n",
    "        # scaler = QuantileTransformer()\n",
    "        features_scaled = scaler.fit_transform(label_df[feature_columns])\n",
    "\n",
    "        # 计算样本间的余弦相似度\n",
    "        similarity_matrix = cosine_similarity(features_scaled)\n",
    "\n",
    "        # 初始化用于存储选定样本索引的集合\n",
    "        selected_samples = set()\n",
    "\n",
    "        # 遍历相似度矩阵，选择相似度高于阈值的样本\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            for j in range(i + 1, len(similarity_matrix)):\n",
    "                if similarity_matrix[i, j] > threshold:\n",
    "                    selected_samples.add(i)\n",
    "                    selected_samples.add(j)\n",
    "\n",
    "        if len(selected_samples) == 0:\n",
    "            print(label_value, f\"共有样本{label_df.shape[0]}个\",\"没有找到相似的样本\")\n",
    "            # 如果selected_samples为空，添加相似度最高的15%的样本\n",
    "            num_to_add = math.ceil(0.75 * num_samples)\n",
    "            if num_to_add > 0:\n",
    "                top_indices = similarity_matrix.sum(axis=0).argsort()[::-1][:num_to_add]\n",
    "                selected_samples.update(top_indices)\n",
    "\n",
    "        # 从原始样本中提取选定的样本\n",
    "        selected_samples_indices = label_df.index[list(selected_samples)]\n",
    "        representative_samples.extend(df.loc[selected_samples_indices].values)\n",
    "\n",
    "# 转换为 DataFrame 并显示部分结果\n",
    "representative_samples_df = pd.DataFrame(representative_samples, columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(representative_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查类别是否丢失\n",
    "old_type = train_df[label_column].value_counts()\n",
    "new_type = representative_samples_df[label_column].value_counts()\n",
    "old_type.shape,new_type.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再合并\n",
    "result_df = pd.concat([representative_samples_df,no_calc_df],ignore_index=True)\n",
    "# result_df = result_df.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['NEW_TZ'].value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = r\"F:\\cache_data\\frequency_filter\\dy\\scaler_csv\"\n",
    "result_df.to_csv(os.path.join(out_path,'dy_stander_filter_all_type_20240417.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = pd.read_csv(r\"F:\\cache_data\\frequency_filter\\dy\\scaler_csv\\dy_stander_filter_all_type_20240417.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类系统更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\Runker\\Desktop\\test\\csv\\point_sample.csv\")\n",
    "df.fillna(df.select_dtypes(include='number').mean(), inplace=True)\n",
    "\n",
    "# Separate points\n",
    "no_calc_df = df[df['label'] == 'inner']\n",
    "train_df = df[df['label'] == 'fish_net']\n",
    "\n",
    "feature_columns = [\n",
    "                   'aligned_Channel Network Base Level',\n",
    "                   'aligned_Channel Network Distance', \n",
    "                'aligned_dem', \n",
    "                   'aligned_ETP2022_mean', 'aligned_evi', 'aligned_LS-Factor',\n",
    "                    'aligned_ndvi',\n",
    "                   'aligned_ndwi', 'aligned_NIGHT2022', 'aligned_pca_1', \n",
    "                 'aligned_PRE2022_mean',\n",
    "                    'aligned_Relative Slope Position',\n",
    "                   'aligned_savi', 'aligned_Slope', 'aligned_TMP2022_mean',\n",
    "                   'aligned_Topographic Wetness Index', 'aligned_Total Catchment Area',\n",
    "                   'aligned_Valley Depth', 'aligned_vari', 'clipped_dem','MRRTF', 'MRVBF', 'slope_postion_101']\n",
    "label_column = 'NEW_TZ'\n",
    "\n",
    "# Feature selection using mutual information\n",
    "mi_scores = mutual_info_classif(train_df[feature_columns], train_df[label_column])\n",
    "mi_scores = pd.Series(mi_scores, index=feature_columns)\n",
    "selected_features = mi_scores.nlargest(15).index.tolist()\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(train_df[selected_features])\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.80)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Adaptive sampling strategy\n",
    "class_counts = Counter(train_df[label_column])\n",
    "sampling_strategy = {cls: max(50, count) for cls, count in class_counts.items()}\n",
    "\n",
    "# Apply random over-sampling\n",
    "ros = RandomOverSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_pca, train_df[label_column])\n",
    "\n",
    "# Function to select representative samples\n",
    "def select_representative_samples(X, y, n_clusters=5):\n",
    "    if len(X) <= n_clusters:\n",
    "        return list(zip(X, y))\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    \n",
    "    representatives = []\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_points = X[clusters == cluster]\n",
    "        cluster_labels = y[clusters == cluster]\n",
    "        if len(cluster_points) > 0:\n",
    "            center = cluster_points.mean(axis=0)\n",
    "            distances = np.sum((cluster_points - center) ** 2, axis=1)\n",
    "            representative_idx = np.argmin(distances)\n",
    "            representatives.append((cluster_points[representative_idx], cluster_labels.iloc[representative_idx]))\n",
    "        else:\n",
    "            # If the cluster is empty, select a random point from X\n",
    "            random_idx = np.random.randint(len(X))\n",
    "            representatives.append((X[random_idx], y.iloc[random_idx]))\n",
    "    \n",
    "    return representatives\n",
    "\n",
    "# Select representative samples for each class\n",
    "representative_samples = []\n",
    "for label_value in np.unique(y_resampled):\n",
    "    X_label = X_resampled[y_resampled == label_value]\n",
    "    y_label = pd.Series(y_resampled[y_resampled == label_value])\n",
    "    n_clusters = min(5, len(X_label))\n",
    "    representatives = select_representative_samples(X_label, y_label, n_clusters=n_clusters)\n",
    "    representative_samples.extend(representatives)\n",
    "\n",
    "# Convert to DataFrame\n",
    "representative_samples_df = pd.DataFrame([sample[0] for sample in representative_samples], \n",
    "                                         columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "representative_samples_df[label_column] = [sample[1] for sample in representative_samples]\n",
    "\n",
    "# Inverse transform PCA and scaling\n",
    "X_original = scaler.inverse_transform(pca.inverse_transform(representative_samples_df.drop(label_column, axis=1)))\n",
    "representative_samples_df[selected_features] = X_original\n",
    "\n",
    "# Merge with no_calc_df\n",
    "result_df = pd.concat([representative_samples_df, no_calc_df], ignore_index=True)\n",
    "\n",
    "# Save results\n",
    "# out_path = r\"F:\\cache_data\\frequency_filter\\dy\\scaler_csv\"\n",
    "# result_df.to_csv(os.path.join(out_path, 'dy_optimized_filter_all_type_20240417.csv'), index=False)\n",
    "\n",
    "# Output statistics\n",
    "print(f\"Original training samples: {len(train_df)}\")\n",
    "print(f\"Representative samples after filtering: {len(representative_samples_df)}\")\n",
    "print(f\"Final result samples (including inner points): {len(result_df)}\")\n",
    "\n",
    "# Check if any categories are lost\n",
    "old_type = train_df[label_column].value_counts()\n",
    "new_type = representative_samples_df[label_column].value_counts()\n",
    "print(f\"Original categories: {len(old_type)}, Categories after filtering: {len(new_type)}\")\n",
    "\n",
    "# Evaluate the quality of the representative samples\n",
    "rf_original = RandomForestClassifier(random_state=42)\n",
    "original_scores = cross_val_score(rf_original, train_df[selected_features], train_df[label_column], cv=5)\n",
    "\n",
    "rf_filtered = RandomForestClassifier(random_state=42)\n",
    "filtered_scores = cross_val_score(rf_filtered, representative_samples_df[selected_features], \n",
    "                                  representative_samples_df[label_column], cv=5)\n",
    "\n",
    "print(f\"Original data cross-validation score: {np.mean(original_scores):.4f} (+/- {np.std(original_scores) * 2:.4f})\")\n",
    "print(f\"Filtered data cross-validation score: {np.mean(filtered_scores):.4f} (+/- {np.std(filtered_scores) * 2:.4f})\")\n",
    "\n",
    "# Visualization (if needed)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=train_df[label_column].astype('category').cat.codes, alpha=0.5)\n",
    "plt.title('Data Distribution after PCA')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar(label='Soil Type Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['TZ'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典\n",
    "json_file_path = 'D:\\worker_code\\Terrain_Test\\data\\soil_dict\\soil_dict.json'\n",
    "# 读取字典\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    loaded_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表\n",
    "df_path = r\"F:\\cache_data\\frequency_filter\\dy\\scaler_csv\\dy_stander_filter_all_type.csv\"\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前的土种信息\n",
    "tz_list = list(df['TZ'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查tz_list中的土种是否在对照字典中\n",
    "check_list = [x in loaded_dict for x in tz_list]\n",
    "in_dict = check_list.count(True)\n",
    "not_in_dict = check_list.count(False)\n",
    "in_dict,not_in_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取现有数据土种字典\n",
    "tz_dict = df.groupby('TZ').apply(lambda x: x[['TL', 'YL', 'TS']].drop_duplicates().to_dict(orient='records')).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfs = []\n",
    "\n",
    "# 遍历字典，将每个分组的数据转换为 DataFrame 并添加到列表中\n",
    "for tz, records in tz_dict.items():\n",
    "    # 将 records 转换为 DataFrame\n",
    "    df_temp = pd.DataFrame(records)\n",
    "    # 添加 'TZ' 列，并设置值为当前的 tz\n",
    "    df_temp['TZ'] = tz\n",
    "    # 将 df_temp 添加到列表中\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# 使用 pd.concat 将所有的 DataFrame 连接起来\n",
    "df_new = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 重置索引\n",
    "df_new.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_excel(r\"C:\\Users\\Runker\\Desktop\\test2.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "features = result_df.drop(columns=['TZ'])\n",
    "# 禁用 FutureWarning 类型的警告\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# 执行代码\n",
    "\n",
    "# 恢复警告设置\n",
    "# warnings.resetwarnings()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 对特征进行标准化\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 应用 PCA\n",
    "pca = PCA(n_components=30)\n",
    "pca_result = pca.fit_transform(features_scaled)\n",
    "\n",
    "# 应用 t-SNE，明确设置 init 为 'pca'\n",
    "tsne = TSNE(n_components=2, random_state=0, init='pca', learning_rate='auto')  # 显式设置 init 和 learning_rate\n",
    "tsne_result = tsne.fit_transform(features_scaled)\n",
    "\n",
    "# 可视化 PCA 和 t-SNE 的结果\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5)\n",
    "plt.title('PCA Result')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.5)\n",
    "plt.title('t-SNE Result')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 返回 PCA 和 t-SNE 结果以供进一步分析\n",
    "pca_result, tsne_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 应用 PCA\n",
    "pca = PCA(n_components=2)  # 使用两个主成分\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 找到在第一主成分上具有最大和最小投影值的样本索引\n",
    "first_pc = X_pca[:, 0]\n",
    "representative_sample_indices = np.argpartition(first_pc, [0, -1])[:2]\n",
    "\n",
    "# 选取具有代表性的样本\n",
    "representative_samples = features.iloc[representative_sample_indices]\n",
    "\n",
    "representative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "# 应用 PCA\n",
    "pca = PCA(n_components=2)  # 降至2维进行可视化\n",
    "pca_result = pca.fit_transform(features)\n",
    "\n",
    "# 计算样本在 PCA 降维后的空间中的坐标\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# 计算样本的密度\n",
    "kde = KernelDensity(bandwidth=2)  # 根据需要调整 bandwidth\n",
    "kde.fit(pca_result)\n",
    "\n",
    "# 计算密度得分\n",
    "density_scores = kde.score_samples(pca_result)\n",
    "\n",
    "# 选择高密度样本\n",
    "threshold = np.percentile(density_scores, 90)  # 根据需要调整阈值\n",
    "high_density_samples = df[density_scores >= threshold]\n",
    "\n",
    "# 显示高密度样本\n",
    "print(high_density_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,QuantileTransformer\n",
    "# label_column = 'TL'\n",
    "\n",
    "# # 获取标签的唯一值\n",
    "# label_values = df[label_column].unique()\n",
    "\n",
    "# # 初始化用于存储代表性样本的列表\n",
    "# representative_samples = []\n",
    "\n",
    "# # 设置相似度阈值\n",
    "# threshold = 0.9  # 根据需要调整\n",
    "\n",
    "# # 遍历每个标签值\n",
    "# for label_value in label_values:\n",
    "#     # 提取当前标签值的样本\n",
    "#     label_df = df[df[label_column] == label_value]\n",
    "#     # 对特征进行标准化\n",
    "#     scaler = StandardScaler()\n",
    "#     # scaler = MinMaxScaler()\n",
    "#     # scaler = MaxAbsScaler()\n",
    "#     # scaler = QuantileTransformer()\n",
    "#     features_scaled = scaler.fit_transform(label_df[feature_columns])\n",
    "\n",
    "#     # 计算样本间的余弦相似度\n",
    "#     similarity_matrix = cosine_similarity(features_scaled)\n",
    "#     print(similarity_matrix)\n",
    "#     # 初始化用于存储选定样本索引的集合\n",
    "#     selected_samples = set()\n",
    "\n",
    "#     # 遍历相似度矩阵，选择相似度高于阈值的样本\n",
    "#     for i in range(len(similarity_matrix)):\n",
    "#         for j in range(i + 1, len(similarity_matrix)):\n",
    "#             if similarity_matrix[i, j] > threshold:\n",
    "#                 selected_samples.add(i)\n",
    "#                 selected_samples.add(j)\n",
    "\n",
    "#     # 从原始样本中提取选定的样本\n",
    "#     selected_samples_indices = label_df.index[list(selected_samples)]\n",
    "#     representative_samples.extend(df.loc[selected_samples_indices].values)\n",
    "\n",
    "# # 转换为 DataFrame 并显示部分结果\n",
    "# representative_samples_df = pd.DataFrame(representative_samples, columns=df.columns)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
