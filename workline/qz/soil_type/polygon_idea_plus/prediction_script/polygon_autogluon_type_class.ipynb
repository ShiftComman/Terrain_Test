{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor,export_graphviz\n",
    "import graphviz\n",
    "import dtreeviz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn import tree\n",
    "from pypinyin import pinyin, lazy_pinyin, Style\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复制一份并改写类别\n",
    "def duplicate_and_rename(df, column_name, new_tz_name):\n",
    "\n",
    "    # 复制一份数据\n",
    "    duplicated_df = df.copy()\n",
    "    # 修改复制的数据的NEW_TZ名称\n",
    "    duplicated_df[column_name] = new_tz_name\n",
    "    # 返回包含原数据和修改后的数据的DataFrame\n",
    "    return pd.concat([df, duplicated_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹已存在\n"
     ]
    }
   ],
   "source": [
    "# autogluon保存路径\n",
    "model_path = r\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\"\n",
    "# 检查路径是否存在，否则便创建\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "else:\n",
    "    print(\"文件夹已存在\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"F:\\cache_data\\zone_ana\\qz\\train_data\\soil_type_train_point.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理前样本总数: 13647\n",
      "处理后样本总数: 16543\n"
     ]
    }
   ],
   "source": [
    "def balance_sample_groups(dataset, group_columns, min_samples=20):\n",
    "    \"\"\"\n",
    "    对数据集中指定列组合的样本数进行平衡处理\n",
    "    \n",
    "    参数:\n",
    "    dataset: pd.DataFrame, 输入的数据集\n",
    "    group_columns: str或list, 用于分组的列名，可以是单个列名或多个列名的列表\n",
    "    min_samples: int, 每个组合的最小样本数，默认为20\n",
    "    \n",
    "    返回:\n",
    "    pd.DataFrame: 平衡后的数据集\n",
    "    \"\"\"\n",
    "    # 确保group_columns是列表\n",
    "    if isinstance(group_columns, str):\n",
    "        group_columns = [group_columns]\n",
    "        \n",
    "    # 复制数据集以避免修改原始数据\n",
    "    dataset = dataset.copy()\n",
    "    \n",
    "    # 统计每个组合的样本数\n",
    "    group_counts = dataset.groupby(group_columns).size().reset_index(name='count')\n",
    "    # 找出样本数少于min_samples的组合\n",
    "    low_count_groups = group_counts[group_counts['count'] < min_samples]\n",
    "    \n",
    "    # 记录处理前的样本总数\n",
    "    original_count = len(dataset)\n",
    "    \n",
    "    # 对每个样本数不足的组合进行处理\n",
    "    for _, group in low_count_groups.iterrows():\n",
    "        # 构建过滤条件\n",
    "        filter_cond = True\n",
    "        for col in group_columns:\n",
    "            filter_cond = filter_cond & (dataset[col] == group[col])\n",
    "            \n",
    "        # 获取当前组合的所有样本\n",
    "        group_samples = dataset[filter_cond]\n",
    "        current_count = len(group_samples)\n",
    "        \n",
    "        # 计算需要复制的样本数\n",
    "        samples_needed = min_samples - current_count\n",
    "        \n",
    "        # 根据当前样本数决定复制方式\n",
    "        if current_count == 1:\n",
    "            samples_to_add = pd.concat([group_samples] * samples_needed)\n",
    "        else:\n",
    "            samples_to_add = group_samples.sample(n=samples_needed, replace=True)\n",
    "            \n",
    "        # 将复制的样本添加到数据集中\n",
    "        dataset = pd.concat([dataset, samples_to_add], ignore_index=True)\n",
    "    \n",
    "    print(f\"处理前样本总数: {original_count}\")\n",
    "    print(f\"处理后样本总数: {len(dataset)}\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 使用示例:\n",
    "# 单个因素\n",
    "# dataset = balance_sample_groups(dataset, 'NEW_TZ')\n",
    "# 多个因素\n",
    "dataset = balance_sample_groups(dataset, ['NEW_TZ', 'MZMC','DL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16543, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"TZ_label\"] = dataset.NEW_TZ.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['dl'] = dataset['dl'].astype('category')\n",
    "dataset['dz'] = dataset['dz'].astype('category')\n",
    "dataset['slopepostion'] = dataset['slopepostion'].astype('category')\n",
    "dataset['TZ_label'] = dataset['TZ_label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '中层壤质灰潮土', 1: '中层壤质红色石灰土', 2: '中层壤质黄色石灰土', 3: '中层壤质黑色石灰土', 4: '中层暗泥质黄壤', 5: '中层泥质黄壤', 6: '中层泥质黄棕壤', 7: '中层灰泥质黄壤', 8: '中层灰泥质黄棕壤', 9: '中层砂泥质黄壤', 10: '中层硅质黄壤', 11: '中层硅质黄棕壤', 12: '中层黏质黄色石灰土', 13: '厚层壤质灰潮土', 14: '厚层泥质黄壤', 15: '厚层灰泥质黄壤', 16: '厚层硅质黄壤', 17: '厚层红泥质黄壤', 18: '朝泥田', 19: '浅石灰泥田', 20: '渗潮泥田', 21: '渗石灰泥田', 22: '潮泥田', 23: '石灰泥田', 24: '紫泥田', 25: '腐中层壤质灰潮土', 26: '腐中层泥质黄壤', 27: '腐中层灰泥质黄壤', 28: '腐中层砂泥质黄壤', 29: '腐中层砂泥质黄壤性土', 30: '腐中层硅质黄壤', 31: '腐厚层壤质灰潮土', 32: '腐厚层红泥质黄壤', 33: '腐薄层壤质红色石灰土', 34: '腐薄层壤质黄色石灰土', 35: '腐薄层壤质黑色石灰土', 36: '腐薄层暗泥质黄壤', 37: '腐薄层暗泥质黄棕壤', 38: '腐薄层泥质黄壤', 39: '腐薄层泥质黄棕壤', 40: '腐薄层泥质黄棕壤性土', 41: '腐薄层灰泥质黄壤', 42: '腐薄层灰泥质黄棕壤', 43: '腐薄层灰泥质黄棕壤性土', 44: '腐薄层砂泥质黄壤', 45: '腐薄层砂泥质黄棕壤', 46: '腐薄层砂泥质黄棕壤性土', 47: '腐薄层砂质酸性紫色土', 48: '腐薄层硅质黄壤', 49: '腐薄层硅质黄壤性土', 50: '腐薄层硅质黄棕壤', 51: '腐薄层硅质黄棕壤性土', 52: '腐薄层黏质黄色石灰土', 53: '薄层壤质红色石灰土', 54: '薄层壤质黄色石灰土', 55: '薄层壤质黑色石灰土', 56: '薄层暗泥质黄棕壤', 57: '薄层泥质黄棕壤', 58: '薄层灰泥质黄壤', 59: '薄层灰泥质黄壤性土', 60: '薄层灰泥质黄棕壤', 61: '薄层灰泥质黄棕壤性土', 62: '薄层砂泥质黄棕壤', 63: '薄层砂泥质黄棕壤性土', 64: '薄层硅质黄壤', 65: '薄层硅质黄壤性土', 66: '薄层硅质黄棕壤', 67: '薄层硅质黄棕壤性土', 68: '薄层黏质黄色石灰土', 69: '轻漂潮泥田', 70: '轻漂灰泥田', 71: '轻漂鳝泥田', 72: '重漂灰泥田', 73: '重漂砂泥田', 74: '青潮泥田', 75: '青石灰泥田', 76: '黄暗泥田', 77: '黄浅白粉泥田', 78: '黄浅砂泥田', 79: '黄浅鳝泥田', 80: '黄渗鳝泥田', 81: '黄白粉泥田', 82: '黄砂泥田', 83: '黄青砂泥田', 84: '黄青鳝泥田', 85: '黄鳝泥田'}\n"
     ]
    }
   ],
   "source": [
    "result = dataset.groupby('TZ_label', observed=True)[\"NEW_TZ\"].apply(lambda x: list(x.unique())).to_dict()\n",
    "for one_type in result:\n",
    "    result[one_type] = result[one_type][0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存映射关系\n",
    "with open(r'D:\\worker_code\\Terrain_Test\\data\\soil_dict_20250227_qz.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLLB</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>自然土</th>\n",
       "      <td>8909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>非自然土</th>\n",
       "      <td>6372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>水稻土</th>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count\n",
       "DLLB       \n",
       "自然土    8909\n",
       "非自然土   6372\n",
       "水稻土    1262"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看地类类别\n",
    "tdlylx_df = pd.DataFrame(dataset['DLLB'].value_counts())\n",
    "tdlylx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>旱地</th>\n",
       "      <td>4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>灌木林地</th>\n",
       "      <td>4042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>乔木林地</th>\n",
       "      <td>3948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>水田</th>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>果园</th>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>其他林地</th>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>其他园地</th>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>其他草地</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>设施农用地</th>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>水浇地</th>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>茶园</th>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>竹林地</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>裸岩石砾地</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>人工牧草地</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>天然牧草地</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>裸土地</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count\n",
       "DL          \n",
       "旱地      4404\n",
       "灌木林地    4042\n",
       "乔木林地    3948\n",
       "水田      1262\n",
       "果园       915\n",
       "其他林地     499\n",
       "其他园地     313\n",
       "其他草地     240\n",
       "设施农用地    240\n",
       "水浇地      200\n",
       "茶园       160\n",
       "竹林地      140\n",
       "裸岩石砾地    120\n",
       "人工牧草地     20\n",
       "天然牧草地     20\n",
       "裸土地       20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看地类类别\n",
    "dl_df = pd.DataFrame(dataset['DL'].value_counts())\n",
    "dl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MZMC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>灰岩</th>\n",
       "      <td>7566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>白云岩</th>\n",
       "      <td>4001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>砂岩</th>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>泥(页)岩</th>\n",
       "      <td>1154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>河流冲积物</th>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>页岩</th>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>粘土岩</th>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>砂页岩</th>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>玄武岩</th>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>泥岩</th>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>第四纪红黏土</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>峨嵋山玄武岩</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>紫色泥(页)岩</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count\n",
       "MZMC          \n",
       "灰岩        7566\n",
       "白云岩       4001\n",
       "砂岩        1704\n",
       "泥(页)岩     1154\n",
       "河流冲积物      526\n",
       "页岩         393\n",
       "粘土岩        377\n",
       "砂页岩        354\n",
       "玄武岩        246\n",
       "泥岩         122\n",
       "第四纪红黏土      40\n",
       "峨嵋山玄武岩      40\n",
       "紫色泥(页)岩     20"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看母质类别\n",
    "mz_df = pd.DataFrame(dataset['MZMC'].value_counts())\n",
    "mz_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砂岩土种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['黄白粉泥田', '黄砂泥田', '黄浅白粉泥田', '重漂砂泥田', '黄青砂泥田', '黄浅砂泥田'], dtype=object),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选砂岩类水稻土数据\n",
    "sy_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                      &(dataset['MZMC'] == '砂岩') ]\n",
    "pd.unique(sy_sdt_data['NEW_TZ']),#pd.unique(sy_sdt_data['NEW_TS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中层硅质黄壤', '厚层硅质黄壤', '薄层硅质黄壤性土', '薄层硅质黄壤', '中层硅质黄棕壤', '薄层硅质黄棕壤',\n",
       "        '薄层硅质黄棕壤性土'], dtype=object),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛砂岩选非自然土数据  \n",
    "sy_fzrt_data = dataset[(dataset['DLLB']=='非自然土') \n",
    "                       & ((dataset['MZMC'] == '砂岩'))]\n",
    "pd.unique(sy_fzrt_data['NEW_TZ']),#pd.unique(sy_fzrt_data['NEW_TS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐中层硅质黄壤', '腐薄层硅质黄壤性土', '腐薄层硅质黄棕壤性土', '腐薄层硅质黄棕壤', '腐薄层硅质黄壤'],\n",
       "       dtype=object),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选自然土数据\n",
    "sy_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                   & ((dataset['MZMC'] == '砂岩') )]\n",
    "pd.unique(sy_zrt_data['NEW_TZ']),#pd.unique(sy_zrt_data['NEW_TS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 碳酸岩土种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['石灰泥田', '青石灰泥田', '渗石灰泥田', '轻漂灰泥田', '浅石灰泥田', '紫泥田', '重漂灰泥田'],\n",
       "       dtype=object),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选碳酸岩水稻土数据\n",
    "tsy_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                      & ((dataset['MZMC'] == '白云岩')|(dataset['MZMC'] == '灰岩'))]\n",
    "pd.unique(tsy_sdt_data['NEW_TZ']),#pd.unique(tsy_sdt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中层灰泥质黄壤', '中层壤质黄色石灰土', '薄层灰泥质黄壤', '薄层黏质黄色石灰土', '薄层壤质黄色石灰土',\n",
       "        '中层壤质黑色石灰土', '厚层灰泥质黄壤', '中层黏质黄色石灰土', '薄层灰泥质黄壤性土', '薄层壤质黑色石灰土',\n",
       "        '中层壤质红色石灰土', '薄层灰泥质黄棕壤', '薄层灰泥质黄棕壤性土', '薄层壤质红色石灰土', '中层灰泥质黄棕壤'],\n",
       "       dtype=object),)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选碳酸岩选非自然土数据\n",
    "tsy_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & ((dataset['MZMC'] == '白云岩')|(dataset['MZMC'] == '灰岩'))]\n",
    "\n",
    "pd.unique(tsy_fzrt_data['NEW_TZ']),#pd.unique(tsy_fzrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐中层灰泥质黄壤', '腐薄层壤质黄色石灰土', '腐薄层壤质黑色石灰土', '腐薄层黏质黄色石灰土', '腐薄层灰泥质黄壤',\n",
       "        '腐薄层壤质红色石灰土', '腐薄层灰泥质黄棕壤', '腐薄层灰泥质黄棕壤性土'], dtype=object),)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选碳酸岩自然土数据\n",
    "\n",
    "tsy_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & ((dataset['MZMC'] == '白云岩')|(dataset['MZMC'] == '灰岩'))]\n",
    "pd.unique(tsy_zrt_data['NEW_TZ']),#pd.unique(tsy_zrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 泥(页)岩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['黄渗鳝泥田', '黄鳝泥田', '轻漂鳝泥田', '黄青鳝泥田', '黄浅鳝泥田'], dtype=object),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选泥(页)岩水稻土数据\n",
    "nyy_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                       & ((dataset['MZMC'] == '泥(页)岩') | (dataset['MZMC'] == '页岩') | (dataset['MZMC'] == '粘土岩') | (dataset['MZMC'] == '板岩') | (dataset['MZMC'] == '泥岩'))]\n",
    "pd.unique(nyy_sdt_data['NEW_TZ']),#pd.unique(nyy_sdt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中层泥质黄壤', '厚层泥质黄壤', '薄层泥质黄棕壤', '薄层砂泥质黄棕壤', '中层泥质黄棕壤'], dtype=object),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选泥(页)岩非自然土数据\n",
    "nyy_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & ((dataset['MZMC'] == '泥(页)岩') | (dataset['MZMC'] == '页岩') | (dataset['MZMC'] == '粘土岩') | (dataset['MZMC'] == '板岩') | (dataset['MZMC'] == '泥岩'))]\n",
    "pd.unique(nyy_fzrt_data['NEW_TZ']),#pd.unique(nyy_fzrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐中层泥质黄壤', '腐薄层泥质黄棕壤', '腐薄层泥质黄壤', '腐薄层泥质黄棕壤性土'], dtype=object),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选泥(页)岩自然土数据    \n",
    "nyy_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & ((dataset['MZMC'] == '泥(页)岩') | (dataset['MZMC'] == '页岩') | (dataset['MZMC'] == '粘土岩') | (dataset['MZMC'] == '板岩') | (dataset['MZMC'] == '泥岩'))]\n",
    "pd.unique(nyy_zrt_data['NEW_TZ']),#pd.unique(nyy_zrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 河流冲积物"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['潮泥田', '青潮泥田', '轻漂潮泥田', '渗潮泥田', '朝泥田'], dtype=object),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选河流冲积物水稻土数据\n",
    "hlcjw_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                       & (dataset['MZMC'] == '河流冲积物')]\n",
    "pd.unique(hlcjw_sdt_data['NEW_TZ']),#pd.unique(hlcjw_sdt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中层壤质灰潮土', '厚层壤质灰潮土'], dtype=object),)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选河流冲积物非自然土数据\n",
    "hlcjw_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & (dataset['MZMC'] == '河流冲积物')]\n",
    "pd.unique(hlcjw_fzrt_data['NEW_TZ']),#pd.unique(hlcjw_fzrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐厚层壤质灰潮土', '腐中层壤质灰潮土'], dtype=object),)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选河流冲积物自然土数据\n",
    "hlcjw_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & (dataset['MZMC'] == '河流冲积物')]\n",
    "pd.unique(hlcjw_zrt_data['NEW_TZ']),#pd.unique(hlcjw_zrt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砂页岩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['黄砂泥田'], dtype=object),)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选砂页岩水稻土数据\n",
    "syy_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                       & ((dataset['MZMC'] == '砂页岩') | (dataset['MZMC'] == '砾岩'))]\n",
    "pd.unique(syy_sdt_data['NEW_TZ']),#pd.unique(syy_sdt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['黄砂泥田', '黄浅砂泥田'], dtype=object),)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复制一份并改写类别\n",
    "syy_sdt_data = duplicate_and_rename(syy_sdt_data, 'NEW_TZ', '黄浅砂泥田')\n",
    "pd.unique(syy_sdt_data['NEW_TZ']),#pd.unique(syy_sdt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中层砂泥质黄壤', '薄层砂泥质黄棕壤', '薄层砂泥质黄棕壤性土'], dtype=object),)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选砂页岩非自然土数据\n",
    "syy_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & ((dataset['MZMC'] == '砂页岩') | (dataset['MZMC'] == '砾岩'))]\n",
    "pd.unique(syy_fzrt_data['NEW_TZ']),#pd.unique(syy_fzrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐薄层砂泥质黄棕壤', '腐薄层砂泥质黄壤', '腐中层砂泥质黄壤', '腐薄层砂泥质黄棕壤性土', '腐中层砂泥质黄壤性土'],\n",
       "       dtype=object),)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选砂页岩物自然土数据\n",
    "syy_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & ((dataset['MZMC'] == '砂页岩') | (dataset['MZMC'] == '砾岩'))]\n",
    "pd.unique(syy_zrt_data['NEW_TZ']),#pd.unique(syy_zrt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 玄武岩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['黄暗泥田'], dtype=object),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选玄武岩水稻土数据\n",
    "xw_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                       & ((dataset['MZMC'] == '玄武岩') | (dataset['MZMC'] == '峨嵋山玄武岩'))]\n",
    "pd.unique(xw_sdt_data['NEW_TZ']),#pd.unique(xw_sdt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['黄暗泥田', '黄渗暗泥田'], dtype=object),)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复制一份并改写类别\n",
    "xw_sdt_data = duplicate_and_rename(xw_sdt_data, 'NEW_TZ', '黄渗暗泥田')\n",
    "pd.unique(xw_sdt_data['NEW_TZ']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐薄层暗泥质黄壤', '腐薄层暗泥质黄棕壤'], dtype=object),)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选玄武岩自然土数据\n",
    "xw_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & ((dataset['MZMC'] == '玄武岩')| (dataset['MZMC'] == '峨嵋山玄武岩'))]\n",
    "pd.unique(xw_zrt_data['NEW_TZ']),#pd.unique(xw_zrt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['中层暗泥质黄壤', '薄层暗泥质黄棕壤'], dtype=object),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选玄武岩非自然土数据\n",
    "xw_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & ((dataset['MZMC'] == '玄武岩')| (dataset['MZMC'] == '峨嵋山玄武岩'))]\n",
    "pd.unique(xw_fzrt_data['NEW_TZ']),#pd.unique(xw_fzrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 红黏土"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=object),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选红黏土水稻土数据\n",
    "hnt_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                       & ((dataset['MZMC'] == '第四纪红黏土') | (dataset['MZMC'] == '红粘土'))]\n",
    "pd.unique(hnt_sdt_data['NEW_TZ']),#pd.unique(hnt_sdt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐厚层红泥质黄壤'], dtype=object),)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选红黏土自然土数据\n",
    "hnt_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & ((dataset['MZMC'] == '第四纪红黏土') | (dataset['MZMC'] == '红粘土'))]\n",
    "pd.unique(hnt_zrt_data['NEW_TZ']),#pd.unique(hnt_zrt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐厚层红泥质黄壤', '腐中层红泥质黄壤'], dtype=object),)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复制一份并改写类别\n",
    "hnt_zrt_data = duplicate_and_rename(hnt_zrt_data, 'NEW_TZ', '腐中层红泥质黄壤')\n",
    "pd.unique(hnt_zrt_data['NEW_TZ']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['厚层红泥质黄壤'], dtype=object),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选红黏土非自然土数据\n",
    "hnt_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & ((dataset['MZMC'] == '第四纪红黏土') | (dataset['MZMC'] == '红粘土'))]\n",
    "pd.unique(hnt_fzrt_data['NEW_TZ']),#pd.unique(hnt_fzrt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['厚层红泥质黄壤', '中层红泥质黄壤'], dtype=object),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复制一份并改写类别\n",
    "hnt_fzrt_data = duplicate_and_rename(hnt_fzrt_data, 'NEW_TZ', '中层红泥质黄壤')\n",
    "pd.unique(hnt_fzrt_data['NEW_TZ']),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 紫色砂页岩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=object),)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选紫色砂页岩水稻土数据\n",
    "zhs_sdt_data = dataset[(dataset['DLLB']=='水稻土')\n",
    "                       & ((dataset['MZMC'] == '紫色泥(页)岩') | (dataset['MZMC'] == '紫红色砂页岩'))]\n",
    "pd.unique(zhs_sdt_data['NEW_TZ']),#pd.unique(zhs_sdt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=object),)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选紫色砂页岩非自然土数据\n",
    "zhs_fzrt_data = dataset[(dataset['DLLB']=='非自然土')\n",
    "                       & ((dataset['MZMC'] == '紫色泥(页)岩') | (dataset['MZMC'] == '紫红色砂页岩'))]\n",
    "pd.unique(zhs_fzrt_data['NEW_TZ']),#pd.unique(zhs_fzrt_data['NEW_TS']),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐薄层砂质酸性紫色土'], dtype=object),)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 筛选紫色砂页岩自然土数据\n",
    "zhs_zrt_data = dataset[(dataset['DLLB']=='自然土')\n",
    "                       & ((dataset['MZMC'] == '紫色泥(页)岩') | (dataset['MZMC'] == '紫红色砂页岩'))]\n",
    "pd.unique(zhs_zrt_data['NEW_TZ']),#pd.unique(zhs_zrt_data['NEW_TS']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['腐薄层砂质酸性紫色土', '腐中层砂质酸性紫色土'], dtype=object),)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复制一份并改写类别\n",
    "zhs_zrt_data = duplicate_and_rename(zhs_zrt_data, 'NEW_TZ', '腐中层砂质酸性紫色土')\n",
    "pd.unique(zhs_zrt_data['NEW_TZ']),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_use = ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', 'contrast', \n",
    "                'convergenceindex', 'correlation', 'dem', 'dissimilarity', 'dl', 'dz', 'entropy', 'etp22_3', 'etp22_mean', \n",
    "                'evi', 'ferrous_minerals', 'hillshade', 'homogeneity', 'lat', 'lon', 'lsfactor', 'lswi', 'mean', 'mndwi', \n",
    "                'mrrtf', 'mrvbf', 'ndmi', 'ndvi', 'ndwi', 'night22_', 'pc1', 'pc2', 'plancurvature', 'pre22_3', 'pre22_mean', \n",
    "                'profilecurvature', 'relativeslopeposition', 'rock_outcrop', 'savi', 'secondmoment', 'slope', 'slopepostion', \n",
    "                'terrainruggednessindex', 'tmp22_3', 'tmp22_mean', 'topographicwetnessindex', 'totalcatchmentarea', 'valleydepth',\n",
    "                'vari', 'variance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sdt_features = features_use\n",
    "fzrt_features = features_use\n",
    "zrt_features = features_use\n",
    "target = \"TZ_label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '中层壤质灰潮土', 1: '中层壤质红色石灰土', 2: '中层壤质黄色石灰土', 3: '中层壤质黑色石灰土', 4: '中层暗泥质黄壤', 5: '中层泥质黄壤', 6: '中层泥质黄棕壤', 7: '中层灰泥质黄壤', 8: '中层灰泥质黄棕壤', 9: '中层砂泥质黄壤', 10: '中层硅质黄壤', 11: '中层硅质黄棕壤', 12: '中层红泥质黄壤', 13: '中层黏质黄色石灰土', 14: '厚层壤质灰潮土', 15: '厚层泥质黄壤', 16: '厚层灰泥质黄壤', 17: '厚层硅质黄壤', 18: '厚层红泥质黄壤', 19: '朝泥田', 20: '浅石灰泥田', 21: '渗潮泥田', 22: '渗石灰泥田', 23: '潮泥田', 24: '石灰泥田', 25: '紫泥田', 26: '腐中层壤质灰潮土', 27: '腐中层泥质黄壤', 28: '腐中层灰泥质黄壤', 29: '腐中层砂泥质黄壤', 30: '腐中层砂泥质黄壤性土', 31: '腐中层砂质酸性紫色土', 32: '腐中层硅质黄壤', 33: '腐中层红泥质黄壤', 34: '腐厚层壤质灰潮土', 35: '腐厚层红泥质黄壤', 36: '腐薄层壤质红色石灰土', 37: '腐薄层壤质黄色石灰土', 38: '腐薄层壤质黑色石灰土', 39: '腐薄层暗泥质黄壤', 40: '腐薄层暗泥质黄棕壤', 41: '腐薄层泥质黄壤', 42: '腐薄层泥质黄棕壤', 43: '腐薄层泥质黄棕壤性土', 44: '腐薄层灰泥质黄壤', 45: '腐薄层灰泥质黄棕壤', 46: '腐薄层灰泥质黄棕壤性土', 47: '腐薄层砂泥质黄壤', 48: '腐薄层砂泥质黄棕壤', 49: '腐薄层砂泥质黄棕壤性土', 50: '腐薄层砂质酸性紫色土', 51: '腐薄层硅质黄壤', 52: '腐薄层硅质黄壤性土', 53: '腐薄层硅质黄棕壤', 54: '腐薄层硅质黄棕壤性土', 55: '腐薄层黏质黄色石灰土', 56: '薄层壤质红色石灰土', 57: '薄层壤质黄色石灰土', 58: '薄层壤质黑色石灰土', 59: '薄层暗泥质黄棕壤', 60: '薄层泥质黄棕壤', 61: '薄层灰泥质黄壤', 62: '薄层灰泥质黄壤性土', 63: '薄层灰泥质黄棕壤', 64: '薄层灰泥质黄棕壤性土', 65: '薄层砂泥质黄棕壤', 66: '薄层砂泥质黄棕壤性土', 67: '薄层硅质黄壤', 68: '薄层硅质黄壤性土', 69: '薄层硅质黄棕壤', 70: '薄层硅质黄棕壤性土', 71: '薄层黏质黄色石灰土', 72: '轻漂潮泥田', 73: '轻漂灰泥田', 74: '轻漂鳝泥田', 75: '重漂灰泥田', 76: '重漂砂泥田', 77: '青潮泥田', 78: '青石灰泥田', 79: '黄暗泥田', 80: '黄浅白粉泥田', 81: '黄浅砂泥田', 82: '黄浅鳝泥田', 83: '黄渗暗泥田', 84: '黄渗鳝泥田', 85: '黄白粉泥田', 86: '黄砂泥田', 87: '黄青砂泥田', 88: '黄青鳝泥田', 89: '黄鳝泥田'}\n"
     ]
    }
   ],
   "source": [
    "# 如果不相等则需要重新合并数据后重新计算TZ_label\n",
    "calc_df = pd.concat([sy_sdt_data,sy_fzrt_data,sy_zrt_data,tsy_sdt_data,tsy_fzrt_data,tsy_zrt_data,nyy_sdt_data,nyy_fzrt_data,nyy_zrt_data,hlcjw_sdt_data,hlcjw_fzrt_data,hlcjw_zrt_data,syy_sdt_data,syy_fzrt_data,syy_zrt_data,xw_sdt_data,xw_zrt_data,xw_fzrt_data,hnt_fzrt_data,hnt_zrt_data,zhs_zrt_data])\n",
    "\n",
    "calc_df[\"TZ_label\"] = calc_df.NEW_TZ.astype(\"category\").cat.codes\n",
    "result = calc_df.groupby('TZ_label', observed=True)[\"NEW_TZ\"].apply(lambda x: list(x.unique())).to_dict()\n",
    "for one_type in result:\n",
    "    result[one_type] = result[one_type][0]\n",
    "print(result)\n",
    "# 保存映射关系\n",
    "with open(r'D:\\worker_code\\Terrain_Test\\data\\soil_dict_20250227_qz.json', 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Runker\\AppData\\Local\\Temp\\ipykernel_24744\\1502417339.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 逐个数据更新TZ_label，根据result\n",
    "for one_data in [sy_sdt_data,sy_fzrt_data,sy_zrt_data,tsy_sdt_data,tsy_fzrt_data,tsy_zrt_data,nyy_sdt_data,nyy_fzrt_data,nyy_zrt_data,hlcjw_sdt_data,hlcjw_fzrt_data,hlcjw_zrt_data,syy_sdt_data,syy_fzrt_data,syy_zrt_data,xw_sdt_data,xw_zrt_data,xw_fzrt_data,hnt_fzrt_data,hnt_zrt_data,zhs_zrt_data]:\n",
    "    one_data['TZ_label'] = one_data['NEW_TZ'].map({v: k for k, v in result.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砂岩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取sdt数据集\n",
    "sy_sdt_data = sy_sdt_data[sdt_features+[target]]\n",
    "# 获取非自然土数据集\n",
    "sy_fzrt_data = sy_fzrt_data[fzrt_features+[target]]\n",
    "# 获取自然土数据集\n",
    "sy_zrt_data = sy_zrt_data[zrt_features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 碳酸岩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取sdt数据集\n",
    "tsy_sdt_data = tsy_sdt_data[sdt_features+[target]]\n",
    "# 获取非自然土数据集\n",
    "tsy_fzrt_data = tsy_fzrt_data[fzrt_features+[target]]\n",
    "# 获取自然土数据集\n",
    "tsy_zrt_data = tsy_zrt_data[zrt_features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 泥(页)岩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取sdt数据集\n",
    "nyy_sdt_data = nyy_sdt_data[sdt_features+[target]]\n",
    "# 获取非自然土数据集\n",
    "nyy_fzrt_data = nyy_fzrt_data[fzrt_features+[target]]\n",
    "# 获取自然土数据集\n",
    "nyy_zrt_data = nyy_zrt_data[zrt_features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 河流冲积物数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取sdt数据集\n",
    "hlcjw_sdt_data = hlcjw_sdt_data[sdt_features+[target]]\n",
    "# 获取非自然土\n",
    "hlcjw_fzrt_data = hlcjw_fzrt_data[fzrt_features+[target]]\n",
    "# 获取自然土\n",
    "hlcjw_zrt_data = hlcjw_zrt_data[zrt_features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 砂页岩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取sdt数据集\n",
    "syy_sdt_data = syy_sdt_data[sdt_features+[target]]\n",
    "# 获取非自然土数据集\n",
    "syy_fzrt_data = syy_fzrt_data[fzrt_features+[target]]\n",
    "# 获取自然土数据集\n",
    "syy_zrt_data = syy_zrt_data[zrt_features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 玄武岩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取水稻土数据集\n",
    "xw_sdt_data = xw_sdt_data[sdt_features+[target]]\n",
    "# 获取峨嵋山玄武岩数据集\n",
    "xw_zrt_data = xw_zrt_data[zrt_features+[target]]\n",
    "# 获取峨嵋山玄武岩非自然土数据集\n",
    "xw_fzrt_data = xw_fzrt_data[fzrt_features+[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取第四纪红粘土数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 获取第四纪红粘土水稻土数据集\n",
    "# hnt_sdt_data = hnt_sdt_data[sdt_features+[target]]\n",
    "# 获取第四纪红粘土非自然土数据集\n",
    "hnt_fzrt_data = hnt_fzrt_data[fzrt_features+[target]]\n",
    "# 获取第四纪红粘土自然土数据集\n",
    "hnt_zrt_data = hnt_zrt_data[zrt_features+[target]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取紫色砂页岩数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 获取紫色砂页岩水稻土数据集\n",
    "# zhs_sdt_data = zhs_sdt_data[sdt_features+[target]]\n",
    "# # 获取紫色砂页岩非自然土数据集\n",
    "# zhs_fzrt_data = zhs_fzrt_data[fzrt_features+[target]]\n",
    "# 获取紫色砂页岩自然土数据集\n",
    "zhs_zrt_data = zhs_zrt_data[zrt_features+[target]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "砂岩 (171, 52) (582, 52) (951, 52)\n",
      "碳酸岩 (582, 52) (4665, 52) (6320, 52)\n",
      "第四系红粘土 (40, 52) (40, 52)\n",
      "泥页岩 (287, 52) (739, 52) (1020, 52)\n",
      "紫红色砂页岩 (40, 52)\n",
      "河流冲积物 (182, 52) (204, 52) (140, 52)\n",
      "砂页岩 (40, 52) (80, 52) (254, 52)\n",
      "玄武岩 (40, 52) (184, 52) (82, 52)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((16643, 52), 16543)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看各个数据集的大小\n",
    "print('砂岩',sy_sdt_data.shape,sy_fzrt_data.shape,sy_zrt_data.shape)\n",
    "print('碳酸岩',tsy_sdt_data.shape,tsy_fzrt_data.shape,tsy_zrt_data.shape)\n",
    "print('第四系红粘土',hnt_fzrt_data.shape,hnt_zrt_data.shape)\n",
    "print('泥页岩',nyy_sdt_data.shape,nyy_fzrt_data.shape,nyy_zrt_data.shape)\n",
    "print('紫红色砂页岩',zhs_zrt_data.shape)\n",
    "print('河流冲积物',hlcjw_sdt_data.shape,hlcjw_fzrt_data.shape,hlcjw_zrt_data.shape)\n",
    "print('砂页岩',syy_sdt_data.shape,syy_fzrt_data.shape,syy_zrt_data.shape)\n",
    "print('玄武岩',xw_sdt_data.shape,xw_zrt_data.shape,xw_fzrt_data.shape)\n",
    "\n",
    "# 计算总数\n",
    "# total_data = pd.concat([sy_sdt_data,sy_fzrt_data,sy_zrt_data,tsy_sdt_data,tsy_fzrt_data,tsy_zrt_data,hnt_sdt_data,hnt_fzrt_data,hnt_zrt_data,nyy_sdt_data,nyy_fzrt_data,nyy_zrt_data,zhsyy_sdt_data,zhsyy_fzrt_data,zhsyy_zrt_data,hlcjw_sdt_data,hlcjw_fzrt_data,hlcjw_zrt_data,syy_sdt_data,syy_fzrt_data,syy_zrt_data,xw_sdt_data,xw_zrt_data,xw_fzrt_data])\n",
    "total_data = pd.concat([sy_sdt_data,sy_fzrt_data,sy_zrt_data,tsy_sdt_data,tsy_fzrt_data,tsy_zrt_data,nyy_sdt_data,nyy_fzrt_data,nyy_zrt_data,hlcjw_sdt_data,hlcjw_fzrt_data,hlcjw_zrt_data,syy_sdt_data,syy_fzrt_data,syy_zrt_data,xw_sdt_data,xw_zrt_data,xw_fzrt_data,hnt_fzrt_data,hnt_zrt_data,zhs_zrt_data])\n",
    "total_data.shape,dataset.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定标签\n",
    "label = \"TZ_label\"\n",
    "eval_metric = 'f1_weighted'\n",
    "problem_type = 'multiclass'\n",
    "# # 10折交叉验证\n",
    "# cv_num = 10\n",
    "# 初始化模型和超参数\n",
    "hyperparameters={\n",
    "\t'NN_TORCH': {},\n",
    "\t'FASTAI': {},\n",
    "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data,label,eval_metric,hyperparameters,problem_type,model_master,model_branch,model_root):\n",
    "    # 训练model\n",
    "    temp_master_path =os.path.join(model_root,model_master)\n",
    "    temp_branch_path = os.path.join(temp_master_path,model_branch)\n",
    "    # 检查路径是否存在，否则便创建\n",
    "    if not os.path.exists(temp_branch_path):\n",
    "        os.makedirs(temp_branch_path)\n",
    "    else:\n",
    "        print(\"文件夹已存在\")\n",
    "    # 执行训练\n",
    "    train_predictor = TabularPredictor(label=label,path=temp_branch_path,eval_metric=eval_metric,problem_type=problem_type).fit(train_data,hyperparameters=hyperparameters)\n",
    "    train_predictor.fit_summary()\n",
    "    return train_predictor.model_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\sdt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\sdt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.32 GB / 63.81 GB (64.8%)\n",
      "Disk Space Avail:   440.27 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    171\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 6\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42310.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])     : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  1 | ['dz']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 136, Val Rows: 35\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.7942\t = Validation score   (f1_weighted)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7907\t = Validation score   (f1_weighted)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7939\t = Validation score   (f1_weighted)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.715\t = Validation score   (f1_weighted)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.7942\t = Validation score   (f1_weighted)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\sdt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   0.794219  f1_weighted       0.000000  2.245455                0.000000           2.245455            1       True          1\n",
      "1  WeightedEnsemble_L2   0.794219  f1_weighted       0.000378  2.309548                0.000378           0.064092            2       True          5\n",
      "2     RandomForestEntr   0.793878  f1_weighted       0.027435  0.460473                0.027435           0.460473            1       True          3\n",
      "3     RandomForestGini   0.790724  f1_weighted       0.033908  0.482544                0.033908           0.482544            1       True          2\n",
      "4       NeuralNetTorch   0.714967  f1_weighted       0.000000  1.283488                0.000000           1.283488            1       True          4\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "('float', [])     : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  1 | ['dz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.09 GB / 63.81 GB (64.4%)\n",
      "Disk Space Avail:   440.27 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    582\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 7\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42073.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])     : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  1 | ['dz']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.21 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 465, Val Rows: 117\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\sdtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.7796\t = Validation score   (f1_weighted)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7903\t = Validation score   (f1_weighted)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7644\t = Validation score   (f1_weighted)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8035\t = Validation score   (f1_weighted)\n",
      "\t2.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.667, 'NeuralNetFastAI': 0.333}\n",
      "\t0.8184\t = Validation score   (f1_weighted)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.67s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\fzrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.09 GB / 63.81 GB (64.4%)\n",
      "Disk Space Avail:   440.26 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    951\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 5\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42068.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.35 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.34 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 760, Val Rows: 191\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2   0.818358  f1_weighted       0.023491  3.390420                0.000000           0.063258            2       True          5\n",
      "1       NeuralNetTorch   0.803510  f1_weighted       0.016022  2.932173                0.016022           2.932173            1       True          4\n",
      "2     RandomForestGini   0.790272  f1_weighted       0.046512  0.490641                0.046512           0.490641            1       True          2\n",
      "3      NeuralNetFastAI   0.779576  f1_weighted       0.007469  0.394989                0.007469           0.394989            1       True          1\n",
      "4     RandomForestEntr   0.764380  f1_weighted       0.034313  0.515999                0.034313           0.515999            1       True          3\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "('float', [])     : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  1 | ['dz']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.8365\t = Validation score   (f1_weighted)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8297\t = Validation score   (f1_weighted)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8396\t = Validation score   (f1_weighted)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8461\t = Validation score   (f1_weighted)\n",
      "\t4.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'RandomForestGini': 0.25, 'NeuralNetTorch': 0.25}\n",
      "\t0.8588\t = Validation score   (f1_weighted)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.3s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\sdt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\sdt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.11 GB / 63.81 GB (64.4%)\n",
      "Disk Space Avail:   440.25 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    582\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 7\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42096.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.21 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 465, Val Rows: 117\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2   0.858832  f1_weighted       0.067138  5.552690                0.000999           0.072842            2       True          5\n",
      "1       NeuralNetTorch   0.846089  f1_weighted       0.011999  4.465636                0.011999           4.465636            1       True          4\n",
      "2     RandomForestEntr   0.839625  f1_weighted       0.037925  0.474189                0.037925           0.474189            1       True          3\n",
      "3      NeuralNetFastAI   0.836470  f1_weighted       0.015986  0.550972                0.015986           0.550972            1       True          1\n",
      "4     RandomForestGini   0.829734  f1_weighted       0.038153  0.463239                0.038153           0.463239            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\sy\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.6836\t = Validation score   (f1_weighted)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7531\t = Validation score   (f1_weighted)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7497\t = Validation score   (f1_weighted)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7135\t = Validation score   (f1_weighted)\n",
      "\t4.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t0.7531\t = Validation score   (f1_weighted)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6.76s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\sdt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.08 GB / 63.81 GB (64.4%)\n",
      "Disk Space Avail:   440.23 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    4665\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 15\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42050.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.72 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     RandomForestGini   0.753090  f1_weighted       0.032049  0.495287                0.032049           0.495287            1       True          2\n",
      "1  WeightedEnsemble_L2   0.753090  f1_weighted       0.033049  0.581045                0.001000           0.085757            2       True          5\n",
      "2     RandomForestEntr   0.749679  f1_weighted       0.032912  0.485979                0.032912           0.485979            1       True          3\n",
      "3       NeuralNetTorch   0.713478  f1_weighted       0.014346  4.852143                0.014346           4.852143            1       True          4\n",
      "4      NeuralNetFastAI   0.683606  f1_weighted       0.008261  0.552005                0.008261           0.552005            1       True          1\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\sdtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.2s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.65 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.28s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.10718113612004287, Train Rows: 4165, Val Rows: 500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.6473\t = Validation score   (f1_weighted)\n",
      "\t5.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7078\t = Validation score   (f1_weighted)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.6987\t = Validation score   (f1_weighted)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6716\t = Validation score   (f1_weighted)\n",
      "\t17.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 0.947, 'NeuralNetFastAI': 0.053}\n",
      "\t0.7131\t = Validation score   (f1_weighted)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 25.99s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\fzrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       40.94 GB / 63.81 GB (64.2%)\n",
      "Disk Space Avail:   440.06 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    6320\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 8\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41921.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2.33 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2   0.713056  f1_weighted       0.062438   6.354354                0.000998           0.126416            2       True          5\n",
      "1     RandomForestGini   0.707773  f1_weighted       0.049442   0.869210                0.049442           0.869210            1       True          2\n",
      "2     RandomForestEntr   0.698737  f1_weighted       0.067000   1.136854                0.067000           1.136854            1       True          3\n",
      "3       NeuralNetTorch   0.671619  f1_weighted       0.018999  17.809037                0.018999          17.809037            1       True          4\n",
      "4      NeuralNetFastAI   0.647296  f1_weighted       0.011999   5.358728                0.011999           5.358728            1       True          1\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 2.24 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 5688, Val Rows: 632\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.6709\t = Validation score   (f1_weighted)\n",
      "\t6.94s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.7231\t = Validation score   (f1_weighted)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.7276\t = Validation score   (f1_weighted)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.6277\t = Validation score   (f1_weighted)\n",
      "\t21.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 0.389, 'RandomForestEntr': 0.389, 'NeuralNetFastAI': 0.222}\n",
      "\t0.7302\t = Validation score   (f1_weighted)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 30.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       40.92 GB / 63.81 GB (64.1%)\n",
      "Disk Space Avail:   439.92 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    40\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41900.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['dl', 'dz', 'slopepostion']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['lswi', 'ndmi', 'pre22_3', 'tmp22_3']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\t\t('int', [])   : 2 | ['pre22_3', 'tmp22_3']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])   :  1 | ['etp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', ['bool']) :  2 | ['etp22_3', 'night22_']\n",
      "\t0.0s = Fit runtime\n",
      "\t44 features in original data used to generate 44 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 32, Val Rows: 8\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L2   0.730226  f1_weighted       0.136898   8.979731                0.001000           0.104126            2       True          5\n",
      "1     RandomForestEntr   0.727560  f1_weighted       0.065578   1.119805                0.065578           1.119805            1       True          3\n",
      "2     RandomForestGini   0.723129  f1_weighted       0.050740   0.819587                0.050740           0.819587            1       True          2\n",
      "3      NeuralNetFastAI   0.670879  f1_weighted       0.019580   6.936212                0.019580           6.936212            1       True          1\n",
      "4       NeuralNetTorch   0.627724  f1_weighted       0.020990  21.198850                0.020990          21.198850            1       True          4\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\tsy\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.25\t = Validation score   (f1_weighted)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.2\t = Validation score   (f1_weighted)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.5\t = Validation score   (f1_weighted)\n",
      "\t0.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.619\t = Validation score   (f1_weighted)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.619\t = Validation score   (f1_weighted)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\fzrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       40.94 GB / 63.81 GB (64.2%)\n",
      "Disk Space Avail:   439.92 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    40\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41917.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 4): ['dz', 'etp22_3', 'slopepostion', 'tmp22_3']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['lswi', 'ndmi', 'pre22_3', 'tmp22_mean']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 3 | ['lswi', 'ndmi', 'tmp22_mean']\n",
      "\t\t('int', [])   : 1 | ['pre22_3']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['dl']\n",
      "\t\t('float', [])    : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  1 | ['dl']\n",
      "\t\t('float', [])     : 39 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['etp22_mean', 'mrvbf', 'pre22_mean']\n",
      "\t0.1s = Fit runtime\n",
      "\t43 features in original data used to generate 43 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       NeuralNetTorch   0.619048  f1_weighted       0.009931  0.213640                0.009931           0.213640            1       True          4\n",
      "1  WeightedEnsemble_L2   0.619048  f1_weighted       0.011933  0.321854                0.002002           0.108214            2       True          5\n",
      "2      NeuralNetFastAI   0.500000  f1_weighted       0.007899  0.298907                0.007899           0.298907            1       True          3\n",
      "3     RandomForestGini   0.250000  f1_weighted       0.031067  0.429245                0.031067           0.429245            1       True          1\n",
      "4     RandomForestEntr   0.200000  f1_weighted       0.033146  0.404048                0.033146           0.404048            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', ['bool']) :  2 | ['etp22_3', 'night22_']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 32, Val Rows: 8\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.1111\t = Validation score   (f1_weighted)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.1111\t = Validation score   (f1_weighted)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 9: early stopping\n",
      "\t0.5\t = Validation score   (f1_weighted)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.2727\t = Validation score   (f1_weighted)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.5\t = Validation score   (f1_weighted)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\sdt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\sdt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       40.94 GB / 63.81 GB (64.1%)\n",
      "Disk Space Avail:   439.92 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    287\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 5\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41917.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.11 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.10 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 229, Val Rows: 58\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   0.500000  f1_weighted       0.009260  0.314888                0.009260           0.314888            1       True          3\n",
      "1  WeightedEnsemble_L2   0.500000  f1_weighted       0.010259  0.382686                0.000999           0.067797            2       True          5\n",
      "2       NeuralNetTorch   0.272727  f1_weighted       0.006000  0.190156                0.006000           0.190156            1       True          4\n",
      "3     RandomForestGini   0.111111  f1_weighted       0.032107  0.391557                0.032107           0.391557            1       True          1\n",
      "4     RandomForestEntr   0.111111  f1_weighted       0.033003  0.422439                0.033003           0.422439            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  1 | ['dl']\n",
      "('float', [])     : 39 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', ['bool']) :  3 | ['etp22_mean', 'mrvbf', 'pre22_mean']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hnt\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.8079\t = Validation score   (f1_weighted)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.8092\t = Validation score   (f1_weighted)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.8224\t = Validation score   (f1_weighted)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8393\t = Validation score   (f1_weighted)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.8393\t = Validation score   (f1_weighted)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\sdt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.01 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.91 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    739\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 5\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41992.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.27 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       NeuralNetTorch   0.839327  f1_weighted       0.009179  2.252882                0.009179           2.252882            1       True          4\n",
      "1  WeightedEnsemble_L2   0.839327  f1_weighted       0.010178  2.337269                0.000999           0.084387            2       True          5\n",
      "2     RandomForestEntr   0.822388  f1_weighted       0.032300  0.502285                0.032300           0.502285            1       True          3\n",
      "3     RandomForestGini   0.809195  f1_weighted       0.033217  0.532981                0.033217           0.532981            1       True          2\n",
      "4      NeuralNetFastAI   0.807916  f1_weighted       0.009269  1.581473                0.009269           1.581473            1       True          1\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\sdtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.2s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.26 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 591, Val Rows: 148\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9803\t = Validation score   (f1_weighted)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9867\t = Validation score   (f1_weighted)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9936\t = Validation score   (f1_weighted)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9936\t = Validation score   (f1_weighted)\n",
      "\t3.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestEntr': 1.0}\n",
      "\t0.9936\t = Validation score   (f1_weighted)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.74s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\fzrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.02 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.91 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    1020\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 4\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41991.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.38 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       NeuralNetTorch   0.993605  f1_weighted       0.015001  3.442005                0.015001           3.442005            1       True          4\n",
      "1     RandomForestEntr   0.993605  f1_weighted       0.033013  0.555276                0.033013           0.555276            1       True          3\n",
      "2  WeightedEnsemble_L2   0.993605  f1_weighted       0.034010  0.628278                0.000998           0.073002            2       True          5\n",
      "3     RandomForestGini   0.986715  f1_weighted       0.048537  0.455822                0.048537           0.455822            1       True          2\n",
      "4      NeuralNetFastAI   0.980293  f1_weighted       0.008000  0.801219                0.008000           0.801219            1       True          1\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 816, Val Rows: 204\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.9414\t = Validation score   (f1_weighted)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9394\t = Validation score   (f1_weighted)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9394\t = Validation score   (f1_weighted)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9322\t = Validation score   (f1_weighted)\n",
      "\t4.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.9414\t = Validation score   (f1_weighted)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 7.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\zhs\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\zhs\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.02 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.90 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    40\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42004.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['dz', 'mrrtf', 'slopepostion']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['dl']\n",
      "\t\t('float', [])    : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 41 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  2 | ['dl', 'mrvbf']\n",
      "\t0.0s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 32, Val Rows: 8\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   0.941413  f1_weighted       0.012998  1.093656                0.012998           1.093656            1       True          1\n",
      "1  WeightedEnsemble_L2   0.941413  f1_weighted       0.013999  1.206259                0.001000           0.112603            2       True          5\n",
      "2     RandomForestGini   0.939429  f1_weighted       0.032008  0.418915                0.032008           0.418915            1       True          2\n",
      "3     RandomForestEntr   0.939429  f1_weighted       0.032614  0.427516                0.032614           0.427516            1       True          3\n",
      "4       NeuralNetTorch   0.932163  f1_weighted       0.015146  4.810817                0.015146           4.810817            1       True          4\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\nyy\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.0\t = Validation score   (f1_weighted)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.0\t = Validation score   (f1_weighted)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.3651\t = Validation score   (f1_weighted)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.2727\t = Validation score   (f1_weighted)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.3651\t = Validation score   (f1_weighted)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\zhs\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\sdt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\sdt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.01 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.90 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    182\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 5\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42001.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   0.365079  f1_weighted       0.005000  0.230130                0.005000           0.230130            1       True          3\n",
      "1  WeightedEnsemble_L2   0.365079  f1_weighted       0.005999  0.297249                0.001000           0.067119            2       True          5\n",
      "2       NeuralNetTorch   0.272727  f1_weighted       0.007824  0.164433                0.007824           0.164433            1       True          4\n",
      "3     RandomForestEntr   0.000000  f1_weighted       0.033010  0.435957                0.033010           0.435957            1       True          2\n",
      "4     RandomForestGini   0.000000  f1_weighted       0.047861  0.367922                0.047861           0.367922            1       True          1\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 41 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  2 | ['dl', 'mrvbf']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\zhs\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTrain Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 145, Val Rows: 37\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t0.6818\t = Validation score   (f1_weighted)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.6\t = Validation score   (f1_weighted)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.5471\t = Validation score   (f1_weighted)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.7233\t = Validation score   (f1_weighted)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 1.0}\n",
      "\t0.7233\t = Validation score   (f1_weighted)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 4.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\sdt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       40.99 GB / 63.81 GB (64.2%)\n",
      "Disk Space Avail:   439.90 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    204\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41964.98 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.08 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       NeuralNetTorch   0.723342  f1_weighted       0.018594  1.417614                0.018594           1.417614            1       True          4\n",
      "1  WeightedEnsemble_L2   0.723342  f1_weighted       0.019593  1.542816                0.001000           0.125202            2       True          5\n",
      "2      NeuralNetFastAI   0.681808  f1_weighted       0.013728  1.309286                0.013728           1.309286            1       True          1\n",
      "3     RandomForestGini   0.599957  f1_weighted       0.032660  0.479558                0.032660           0.479558            1       True          2\n",
      "4     RandomForestEntr   0.547104  f1_weighted       0.032443  0.544030                0.032443           0.544030            1       True          3\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\sdtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 163, Val Rows: 41\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 6: early stopping\n",
      "\t0.9516\t = Validation score   (f1_weighted)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9757\t = Validation score   (f1_weighted)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\fzrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.02 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    140\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42006.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.05 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.0s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.05 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.07s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 112, Val Rows: 28\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     RandomForestEntr   1.000000  f1_weighted       0.031327  0.359730                0.031327           0.359730            1       True          2\n",
      "1     RandomForestGini   1.000000  f1_weighted       0.032818  0.391383                0.032818           0.391383            1       True          1\n",
      "2  WeightedEnsemble_L2   1.000000  f1_weighted       0.033816  0.518300                0.000998           0.126917            2       True          5\n",
      "3       NeuralNetTorch   0.975730  f1_weighted       0.017000  0.949840                0.017000           0.949840            1       True          4\n",
      "4      NeuralNetFastAI   0.951638  f1_weighted       0.010704  1.211004                0.010704           1.211004            1       True          3\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 3: early stopping\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\sdt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\sdt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.01 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    40\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41994.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['dl', 'dz']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  1 | ['slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  1 | ['slopepostion']\n",
      "\t\t('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  1 | ['mrrtf']\n",
      "\t0.1s = Fit runtime\n",
      "\t47 features in original data used to generate 47 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 32, Val Rows: 8\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI        1.0  f1_weighted       0.013002  0.713151                0.013002           0.713151            1       True          3\n",
      "1       NeuralNetTorch        1.0  f1_weighted       0.015743  0.549494                0.015743           0.549494            1       True          4\n",
      "2     RandomForestGini        1.0  f1_weighted       0.032838  0.371698                0.032838           0.371698            1       True          1\n",
      "3  WeightedEnsemble_L2        1.0  f1_weighted       0.033840  0.474313                0.001002           0.102615            2       True          5\n",
      "4     RandomForestEntr        1.0  f1_weighted       0.049820  0.343562                0.049820           0.343562            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\hlcjw\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.2727\t = Validation score   (f1_weighted)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.2727\t = Validation score   (f1_weighted)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 9: early stopping\n",
      "\t0.4667\t = Validation score   (f1_weighted)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.3651\t = Validation score   (f1_weighted)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.4667\t = Validation score   (f1_weighted)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.86s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\sdt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.08 GB / 63.81 GB (64.4%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    80\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    42062.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['dz']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  1 | ['mrrtf']\n",
      "\t0.1s = Fit runtime\n",
      "\t48 features in original data used to generate 48 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 64, Val Rows: 16\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   0.466667  f1_weighted       0.007504  0.368484                0.007504           0.368484            1       True          3\n",
      "1  WeightedEnsemble_L2   0.466667  f1_weighted       0.008504  0.440439                0.001000           0.071954            2       True          5\n",
      "2       NeuralNetTorch   0.365079  f1_weighted       0.007999  0.253597                0.007999           0.253597            1       True          4\n",
      "3     RandomForestEntr   0.272727  f1_weighted       0.033217  0.456613                0.033217           0.456613            1       True          2\n",
      "4     RandomForestGini   0.272727  f1_weighted       0.051000  0.456344                0.051000           0.456344            1       True          1\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  1 | ['slopepostion']\n",
      "('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  1 | ['mrrtf']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\sdtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No improvement since epoch 4: early stopping\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\fzrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.00 GB / 63.81 GB (64.2%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    254\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 5\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41980.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.09 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])     : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  1 | ['dz']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.09 MB (0.0% of available memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       NeuralNetTorch        1.0  f1_weighted       0.009001  0.463055                0.009001           0.463055            1       True          4\n",
      "1      NeuralNetFastAI        1.0  f1_weighted       0.010000  0.802175                0.010000           0.802175            1       True          1\n",
      "2  WeightedEnsemble_L2        1.0  f1_weighted       0.012009  0.932279                0.002009           0.130104            2       True          5\n",
      "3     RandomForestEntr        1.0  f1_weighted       0.033022  0.388699                0.033022           0.388699            1       True          3\n",
      "4     RandomForestGini        1.0  f1_weighted       0.035372  0.465007                0.035372           0.465007            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  1 | ['mrrtf']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 203, Val Rows: 51\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 8: early stopping\n",
      "\t0.8995\t = Validation score   (f1_weighted)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.959\t = Validation score   (f1_weighted)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9367\t = Validation score   (f1_weighted)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.8799\t = Validation score   (f1_weighted)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t0.959\t = Validation score   (f1_weighted)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.86s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\sdt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\sdt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.00 GB / 63.81 GB (64.2%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    40\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41981.41 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['dl', 'dz']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['lswi', 'ndmi', 'slopepostion']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('category', []) : 1 | ['slopepostion']\n",
      "\t\t('float', [])    : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])   :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 40 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  2 | ['etp22_3', 'pre22_3']\n",
      "\t\t('int', ['bool']) :  4 | ['entropy', 'mrrtf', 'secondmoment', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 32, Val Rows: 8\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     RandomForestGini   0.959015  f1_weighted       0.046672  0.690329                0.046672           0.690329            1       True          2\n",
      "1  WeightedEnsemble_L2   0.959015  f1_weighted       0.047671  0.796329                0.000999           0.106001            2       True          5\n",
      "2     RandomForestEntr   0.936652  f1_weighted       0.047983  0.568005                0.047983           0.568005            1       True          3\n",
      "3      NeuralNetFastAI   0.899537  f1_weighted       0.021230  1.444572                0.021230           1.444572            1       True          1\n",
      "4       NeuralNetTorch   0.879921  f1_weighted       0.012999  2.720321                0.012999           2.720321            1       True          4\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "('float', [])     : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  1 | ['dz']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\syy\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.1111\t = Validation score   (f1_weighted)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.1111\t = Validation score   (f1_weighted)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 2: early stopping\n",
      "\t0.5636\t = Validation score   (f1_weighted)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.3333\t = Validation score   (f1_weighted)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t0.5636\t = Validation score   (f1_weighted)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1.72s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\sdt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\zrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\zrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       41.00 GB / 63.81 GB (64.3%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    184\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41988.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   0.563636  f1_weighted       0.005056  0.332714                0.005056           0.332714            1       True          3\n",
      "1  WeightedEnsemble_L2   0.563636  f1_weighted       0.007059  0.428054                0.002003           0.095340            2       True          5\n",
      "2       NeuralNetTorch   0.333333  f1_weighted       0.011370  0.235906                0.011370           0.235906            1       True          4\n",
      "3     RandomForestGini   0.111111  f1_weighted       0.031060  0.402154                0.031060           0.402154            1       True          1\n",
      "4     RandomForestEntr   0.111111  f1_weighted       0.033122  0.411156                0.033122           0.411156            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])     : 40 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  2 | ['etp22_3', 'pre22_3']\n",
      "('int', ['bool']) :  4 | ['entropy', 'mrrtf', 'secondmoment', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\sdtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t0.1s = Fit runtime\n",
      "\t49 features in original data used to generate 49 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.07 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 147, Val Rows: 37\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n",
      "\t0.9727\t = Validation score   (f1_weighted)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t0.9459\t = Validation score   (f1_weighted)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t0.9732\t = Validation score   (f1_weighted)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 1.0}\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.12s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\zrt\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\fzrt\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\fzrt\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.10.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          20\n",
      "Memory Avail:       40.99 GB / 63.81 GB (64.2%)\n",
      "Disk Space Avail:   439.89 GB / 1406.25 GB (31.3%)\n",
      "===================================================\n",
      "Train Data Rows:    82\n",
      "Train Data Columns: 51\n",
      "Label Column:       TZ_label\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Train Data Class Count: 2\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    41973.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 1): ['dz']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 2): ['lswi', 'ndmi']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 2 | ['lswi', 'ndmi']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "\t\t('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "\t\t('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "\t\t('int', ['bool']) :  1 | ['mrrtf']\n",
      "\t0.1s = Fit runtime\n",
      "\t48 features in original data used to generate 48 features in processed data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI   1.000000  f1_weighted       0.013124  0.873679                0.013124           0.873679            1       True          3\n",
      "1  WeightedEnsemble_L2   1.000000  f1_weighted       0.014124  0.996706                0.001000           0.123027            2       True          5\n",
      "2       NeuralNetTorch   0.973186  f1_weighted       0.010514  1.038169                0.010514           1.038169            1       True          4\n",
      "3     RandomForestGini   0.972708  f1_weighted       0.049019  0.478963                0.049019           0.478963            1       True          1\n",
      "4     RandomForestEntr   0.945946  f1_weighted       0.033335  0.406403                0.033335           0.406403            1       True          2\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) :  3 | ['dl', 'dz', 'slopepostion']\n",
      "('float', [])    : 43 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])      :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\zrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTrain Data (Processed) Memory Usage: 0.03 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_weighted'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 65, Val Rows: 17\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}],\n",
      "}\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: RandomForestGini ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.47s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ...\n",
      "No improvement since epoch 2: early stopping\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'RandomForestGini': 1.0}\n",
      "\t1.0\t = Validation score   (f1_weighted)\n",
      "\t0.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 2.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\fzrt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val  eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      NeuralNetFastAI        1.0  f1_weighted       0.011583  0.507472                0.011583           0.507472            1       True          3\n",
      "1       NeuralNetTorch        1.0  f1_weighted       0.013000  0.506984                0.013000           0.506984            1       True          4\n",
      "2     RandomForestEntr        1.0  f1_weighted       0.032362  0.473205                0.032362           0.473205            1       True          2\n",
      "3     RandomForestGini        1.0  f1_weighted       0.048950  0.525296                0.048950           0.525296            1       True          1\n",
      "4  WeightedEnsemble_L2        1.0  f1_weighted       0.049950  0.627526                0.001000           0.102230            2       True          5\n",
      "Number of models trained: 5\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'RFModel', 'NNFastAiTabularModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  :  2 | ['dl', 'slopepostion']\n",
      "('float', [])     : 42 | ['aspect', 'carbonate', 'channelnetworkbaselevel', 'channelnetworkdistance', 'clay_minerals', ...]\n",
      "('int', [])       :  3 | ['etp22_3', 'pre22_3', 'tmp22_3']\n",
      "('int', ['bool']) :  1 | ['mrrtf']\n",
      "Plot summary of models saved to file: F:\\cache_data\\zone_ana\\qz\\modle\\autogluon_type\\xw\\fzrtSummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "# 砂岩\n",
    "sy_sdt_predictor = train_model(sy_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='sy',model_branch='sdt',model_root=model_path) \n",
    "sy_fzrt_predictor = train_model(sy_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='sy',model_branch='fzrt',model_root=model_path) \n",
    "sy_zrt_predictor = train_model(sy_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='sy',model_branch='zrt',model_root=model_path) \n",
    "# 碳酸岩\n",
    "tsy_sdt_predictor = train_model(tsy_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='tsy',model_branch='sdt',model_root=model_path) \n",
    "tsy_fzrt_predictor = train_model(tsy_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='tsy',model_branch='fzrt',model_root=model_path) \n",
    "tsy_zrt_predictor = train_model(tsy_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='tsy',model_branch='zrt',model_root=model_path) \n",
    "# 第四系红粘土\n",
    "# hnt_sdt_predictor = train_model(hnt_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='hnt',model_branch='sdt',model_root=model_path) \n",
    "hnt_fzrt_predictor = train_model(hnt_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='hnt',model_branch='fzrt',model_root=model_path) \n",
    "hnt_zrt_predictor = train_model(hnt_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='hnt',model_branch='zrt',model_root=model_path) \n",
    "# 泥页岩\n",
    "nyy_sdt_predictor = train_model(nyy_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='nyy',model_branch='sdt',model_root=model_path) \n",
    "nyy_fzrt_predictor = train_model(nyy_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='nyy',model_branch='fzrt',model_root=model_path) \n",
    "nyy_zrt_predictor = train_model(nyy_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='nyy',model_branch='zrt',model_root=model_path) \n",
    "# 紫红色砂页岩\n",
    "# zhsyy_sdt_predictor = train_model(zhsyy_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='zhsyy',model_branch='sdt',model_root=model_path) \n",
    "# zhsyy_fzrt_predictor = train_model(zhs_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='zhsyy',model_branch='fzrt',model_root=model_path) \n",
    "zhsyy_zrt_predictor = train_model(zhs_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='zhs',model_branch='zrt',model_root=model_path) \n",
    "# 河流冲积物\n",
    "hlcjw_sdt_predictor = train_model(hlcjw_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='hlcjw',model_branch='sdt',model_root=model_path)\n",
    "hlcjw_fzrt_predictor = train_model(hlcjw_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='hlcjw',model_branch='fzrt',model_root=model_path)\n",
    "hlcjw_zrt_predictor = train_model(hlcjw_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='hlcjw',model_branch='zrt',model_root=model_path)\n",
    "# 砂页岩\n",
    "syy_sdt_predictor = train_model(syy_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='syy',model_branch='sdt',model_root=model_path) \n",
    "syy_fzrt_predictor = train_model(syy_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='syy',model_branch='fzrt',model_root=model_path) \n",
    "syy_zrt_predictor = train_model(syy_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='syy',model_branch='zrt',model_root=model_path) \n",
    "# 玄武岩\n",
    "xw_sdt_predictor = train_model(xw_sdt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='xw',model_branch='sdt',model_root=model_path) \n",
    "xw_zrt_predictor = train_model(xw_zrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='xw',model_branch='zrt',model_root=model_path) \n",
    "xw_fzrt_predictor = train_model(xw_fzrt_data,label=label,eval_metric=eval_metric,hyperparameters=hyperparameters,problem_type=problem_type,model_master='xw',model_branch='fzrt',model_root=model_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
