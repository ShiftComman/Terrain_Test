{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arcpy import env\n",
    "from arcpy.management import *\n",
    "from arcpy.conversion import *\n",
    "from arcpy.da import *\n",
    "from arcpy.sa import *\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.svm import SVR,SVC\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix,roc_auc_score,roc_curve,precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV\n",
    "from autogluon.tabular import TabularPredictor,TabularDataset\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析函数\n",
    "# 取消并行处理\n",
    "def disable_parallel_processing(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with arcpy.EnvManager(parallelProcessingFactor=\"0\"):\n",
    "            return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "# 采样\n",
    "def sample_point(point_,raster_,out_name):\n",
    "    \"\"\"根据栅格采样点,输出为表格\"\"\"\n",
    "    Sample(raster_,point_,out_name,\"NEAREST\", \"OBJECTID\", \"CURRENT_SLICE\", None, '', None, None, \"ROW_WISE\", \"TABLE\")\n",
    "    return None\n",
    "\n",
    "# 导出CSV\n",
    "def export_csv(table_,out_path,out_name):\n",
    "    TableToTable(table_,out_path,out_name)\n",
    "    return None\n",
    "# 掩膜提取\n",
    "def mask_raster(array,mask_ele,cell_size):\n",
    "    \"\"\"按掩膜提取栅格,空间参考设定为:CGCS2000_3_Degree_GK_CM_108E\"\"\"\n",
    "    out_raster = arcpy.NumPyArrayToRaster(\n",
    "    array,\n",
    "    arcpy.Point(arcpy.env.extent.XMin, arcpy.env.extent.YMin),\n",
    "    cell_size,\n",
    "    cell_size,\n",
    ")\n",
    "    \"\"\"按掩膜提取栅格,空间参考设定为:CGCS2000_3_Degree_GK_CM_108E\"\"\"\n",
    "    output_coordinate_system = arcpy.Describe(mask_ele).spatialReference\n",
    "    with arcpy.EnvManager(outputCoordinateSystem=output_coordinate_system,snapRaster=mask_ele, cellSize=mask_ele):\n",
    "        result_raster = arcpy.sa.ExtractByMask(out_raster, mask_ele, \"INSIDE\")\n",
    "        return result_raster\n",
    "# 数组整形\n",
    "def resize_arrays(A, B, fill_value=0):\n",
    "    \"\"\"调整数组形状一致,A为参考数组, B为待调整数组, fill_value为填充值\"\"\"\n",
    "    # new_shape = (max(A.shape[0], B.shape[0]), max(A.shape[1], B.shape[1]))\n",
    "    new_shape = (A.shape[0], A.shape[1])\n",
    "\n",
    "    if A.shape != new_shape:\n",
    "        if A.shape[0] < new_shape[0]:\n",
    "            padding_rows = new_shape[0] - A.shape[0]\n",
    "            padding = np.full((padding_rows, A.shape[1]), fill_value)\n",
    "            A = np.vstack((A, padding))\n",
    "        elif A.shape[0] > new_shape[0]:\n",
    "            A = A[:new_shape[0], :]\n",
    "\n",
    "        if A.shape[1] < new_shape[1]:\n",
    "            pad_width = ((0, 0), (0, new_shape[1] - A.shape[1]))\n",
    "            A = np.pad(A, pad_width, mode='constant', constant_values=fill_value)\n",
    "        elif A.shape[1] > new_shape[1]:\n",
    "            A = A[:, :new_shape[1]]\n",
    "    \n",
    "    if B.shape != new_shape:\n",
    "        if B.shape[0] < new_shape[0]:\n",
    "            padding_rows = new_shape[0] - B.shape[0]\n",
    "            padding = np.full((padding_rows, B.shape[1]), fill_value)\n",
    "            B = np.vstack((B, padding))\n",
    "        elif B.shape[0] > new_shape[0]:\n",
    "            B = B[:new_shape[0], :]\n",
    "\n",
    "        if B.shape[1] < new_shape[1]:\n",
    "            pad_width = ((0, 0), (0, new_shape[1] - B.shape[1]))\n",
    "            B = np.pad(B, pad_width, mode='constant', constant_values=fill_value)\n",
    "        elif B.shape[1] > new_shape[1]:\n",
    "            B = B[:, :new_shape[1]]\n",
    "    \n",
    "    return A, B\n",
    "# rf寻找最优参数\n",
    "def rf_best_param(X_train,y_train,n_estimators_range,k=5):\n",
    "    \"\"\"默认为5折交叉验证,评价指标为R2\"\"\"\n",
    "    # 设置树的数目范围\n",
    "    n_estimators_range = n_estimators_range\n",
    "    cv_scores = []\n",
    "    # 使用交叉验证\n",
    "    for n_estimators in n_estimators_range:\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "        scores = cross_val_score(rf,X_train, y_train, cv=k, scoring='accuracy')  # K折交叉验证 分类问题默认使用accuracy\n",
    "        cv_scores.append(scores.mean())\n",
    "    # 选择最优数量的树\n",
    "    optimal_n_estimators = n_estimators_range[cv_scores.index(max(cv_scores))]\n",
    "    return optimal_n_estimators\n",
    "\n",
    "# arcpy栅格众数滤波\n",
    "@disable_parallel_processing\n",
    "def mode_filter(in_raster,filter_size=\"FOUR\",filter_method=\"MAJORITY\"):\n",
    "    \"\"\"栅格众数滤波,filter_size:滤波窗口大小,filter_method:滤波方法\"\"\"\n",
    "    result_raster = MajorityFilter(in_raster, filter_size, filter_method) # 默认为4邻域 众数滤波  可选为8邻域 半数滤波\n",
    "    return result_raster\n",
    "    \n",
    "# arcpy栅格边界清理\n",
    "@disable_parallel_processing\n",
    "def clean_boundary(in_raster,sort_method=\"NO_SORT\",clean_method=\"TWO_WAY\"):\n",
    "    \"\"\"栅格边界清理,sort_method:排序方法,clean_method:清理方法\"\"\"\n",
    "    result_raster = BoundaryClean(in_raster, \"NO_SORT\", \"TWO_WAY\") # 默认为不排序 两边清理 可选为排序 一边清理\n",
    "    return result_raster\n",
    "# arcpy栅格区域分组\n",
    "@disable_parallel_processing\n",
    "def region_group(in_raster,group_size=\"FOUR\",method=\"WITHIN\",add_link=\"ADD_LINK\",clear=None):\n",
    "    \"\"\"栅格区域分组,group_size:分组大小,method:分组方法,add_link:添加链接,clear:排除的值\"\"\"\n",
    "    result_raster = RegionGroup(in_raster, group_size, method, add_link, clear) # 默认为4邻域 分组方法为WITHIN 可选为8邻域 分组方法为CROSS\n",
    "    return result_raster\n",
    "# arcpy栅格按属性提取\n",
    "@disable_parallel_processing\n",
    "def extract_by_attributes(in_raster,where_clause=\"count >= 100\"):\n",
    "    \"\"\"栅格按属性提取,where_clause:属性条件\"\"\"\n",
    "    result_raster = ExtractByAttributes(in_raster, where_clause) # 默认为count >= 100\n",
    "    return result_raster\n",
    "\n",
    "# arcpy栅格Nibble\n",
    "@disable_parallel_processing\n",
    "def nibble(in_raster,mask_raster, nibble_mask=\"ALL_VALUES\", nibble_values=\"PRESERVE_NODATA\"):\n",
    "    \"\"\"栅格Nibble,nibble_mask:掩膜,nibble_values:填充值\"\"\"\n",
    "    result_raster = Nibble(in_raster, mask_raster,nibble_mask, nibble_values) # 默认为ALL_VALUES PRESERVE_NODATA 可选为DATA_ONLY PRESERVE_DATA\n",
    "    return result_raster\n",
    "\n",
    "# arcpy栅格查找表\n",
    "@disable_parallel_processing\n",
    "def lookup(in_raster,lookup_field=\"LINK\"):\n",
    "    \"\"\"栅格查找表,lookup_field:查找字段\"\"\"\n",
    "    result_raster = Lookup(in_raster, lookup_field) # 默认为LINK\n",
    "    return result_raster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 地理数据库路径\n",
    "base_gdb_5m = r\"D:\\ArcgisData\\basedata\\basetrain_5m.gdb\"\n",
    "base_gdb_30m = r\"D:\\ArcgisData\\basedata\\basetrain_30m.gdb\"\n",
    "# 数据点文件路径\n",
    "point_data = r\"D:\\ArcgisData\\pred_tl\\pred_database\\pred_tl.gdb\\黄壤\"\n",
    "# 存储采样数据表的文件地理数据库\n",
    "sample_gdb_path = r\"D:\\ArcgisData\\pred_organic_p_n\\feature_table\\tableresult_autogluon_hr.gdb\"\n",
    "# 存储采样结果CSV文件的路径\n",
    "sample_csv = r\"D:\\ArcgisData\\pred_organic_p_n\\feature_table\\feature_table_result\"\n",
    "# 输出CSV文件的名称\n",
    "sample_csv_name = \"feature_table_result_hr.csv\"\n",
    "# 目标标签\n",
    "target_label = \"土种\"\n",
    "# 随机森林树的范围\n",
    "n_estimators_range = range(10, 2000, 10)\n",
    "# rf模型存储路径\n",
    "modle_save_path = r\"D:\\ArcgisData\\pred_tl\\pred_moudle\\autogluon_hr\"\n",
    "# 栅格输出标准化的数据库\n",
    "stander_raster_gdb = base_gdb_5m\n",
    "# 标准化待预测数据分割路径\n",
    "cut_csv_path = r\"D:\\ArcgisData\\pred_tl\\pred_table\\autogluon\\cut_csv\"\n",
    "# 预测完成CSV文件存储路径\n",
    "pred_csv_path = r\"D:\\ArcgisData\\pred_tl\\pred_table\\autogluon\\pre_csv_hr\"\n",
    "# 完整预测数据存储路径\n",
    "merge_csv_path = r\"D:\\ArcgisData\\pred_tl\\pred_table\\autogluon\\merge_csv_hr\"\n",
    "# 完整预测数据存储名称\n",
    "merge_csv_name = \"merge_hr.csv\"\n",
    "# 栅格输出预测结果数据库\n",
    "pred_raster_gdb = r\"D:\\ArcgisData\\pred_tl\\pred_database\\TL_basedata.gdb\"\n",
    "# 栅格输出预测结果栅格名称\n",
    "pred_raster_name = \"TL_HR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OBJECTID', 'Shape', '亚类', '土属', '土种', '土类']\n",
      "['OBJECTID', '亚类', '土属', '土种', '土类']\n"
     ]
    }
   ],
   "source": [
    "# 采样点数据名称\n",
    "sample_name = '黄壤'\n",
    "filed_list = [_.name for _ in arcpy.ListFields(point_data)]\n",
    "print(filed_list)\n",
    "elements_yes = ['OBJECTID', '亚类', '土类','土属','土种']\n",
    "filter_list = [_ for _ in filed_list if _ in elements_yes]\n",
    "print(filter_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定指标\n",
    "# 指标列表['TWI5','TPI201','TPI101','TPI11','TPI3','TMP','SOILQS','SLOP','PRE','NIGTH','NDVI','DEM','CUR','ASP','PLCUR','POCUR','OSJL','LAT','LON','DZ','DL','SOM','SC','PH']\n",
    "feature_list = ['LAT','LON','DZ','DL','PH','SOM','SC2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练点数据集采样并输出到csv文件\n",
    "# 选择用于采样的数据库\n",
    "env.workspace = stander_raster_gdb # 切换工作空间用于采样\n",
    "# 选择用于采样的要素类\n",
    "point_data = point_data\n",
    "# 使用Delete_management函数删除数据库中的所有内容\n",
    "try:\n",
    "    arcpy.Delete_management(sample_gdb_path)\n",
    "except:\n",
    "    pass\n",
    "# 再创建一个新的数据库\n",
    "arcpy.management.CreateFileGDB(os.path.dirname(sample_gdb_path), \"tableresult_autogluon_hr\", \"CURRENT\")\n",
    "# 逐个采样并保存到csv文件\n",
    "for one_feature in feature_list:\n",
    "    sample_point(point_data,one_feature,os.path.join(sample_gdb_path,one_feature))\n",
    "env.workspace = os.path.join(sample_gdb_path) # 切换工作空间用于导出csv文件\n",
    "# 读取数据表并保存到csv文件\n",
    "result_df = pd.DataFrame(arcpy.da.FeatureClassToNumPyArray(point_data,filter_list))\n",
    "result_df.rename(columns={\"OBJECTID\":sample_name},inplace=True)\n",
    "#  读取每个表的最后一个字段的数据,存储每个表的最后一个字段的数据\n",
    "for table in feature_list:\n",
    "    # 将表转换为pandas数据帧\n",
    "    df = pd.DataFrame(arcpy.da.TableToNumPyArray(table, \"*\"))  # 确保数据表中无空值\n",
    "    # 提取最后一个字段的数据\n",
    "    merged_df = df[[sample_name, df.columns[-1]]]\n",
    "    # 合并\n",
    "    result_df = pd.merge(result_df, merged_df, on=[sample_name])\n",
    "# 保存到csv文件\n",
    "result_df.rename(columns=dict(zip(result_df.columns[-len(feature_list):], feature_list)),inplace=True)\n",
    "result_df.drop(result_df.columns[0],axis=1,inplace=True)\n",
    "result_df.to_csv(os.path.join(sample_csv,sample_csv_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['good_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=6, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"D:\\ArcgisData\\pred_tl\\pred_moudle\\autogluon_hr\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "Disk Space Avail:   537.85 GB / 808.56 GB (66.5%)\n",
      "Train Data Rows:    615\n",
      "Train Data Columns: 7\n",
      "Label Column: 土种\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 36) unique label values:  ['低灰泥质黄壤', '低泥质黄壤', '中灰泥质黄壤', '中泥质黄壤', '薄层灰泥质黄壤性土', '中泥质黄壤性土', '中灰泥质黄壤性土', '中硅质黄壤', '中层厚层硅质黄壤', '低灰泥质黄壤性土']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Updated label_count_threshold from 10 to 3 to avoid cutting too many classes.\n",
      "Warning: Some classes in the training set have fewer than 3 examples. AutoGluon will only keep 32 out of 36 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 3 examples that will be kept for training models: 0.9902439024390244\n",
      "Train Data Class Count: 32\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    33372.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.13 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 4 | ['LAT', 'LON', 'PH', 'SOM']\n",
      "\t\t('object', []) : 3 | ['DZ', 'DL', 'SC2']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 3 | ['DZ', 'DL', 'SC2']\n",
      "\t\t('float', [])    : 4 | ['LAT', 'LON', 'PH', 'SOM']\n",
      "\t0.0s = Fit runtime\n",
      "\t7 features in original data used to generate 7 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.02 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 299.94s of the 299.94s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.5304\t = Validation score   (accuracy)\n",
      "\t2.82s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 290.8s of the 290.8s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6338\t = Validation score   (accuracy)\n",
      "\t27.38s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 262.36s of the 262.36s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.5764\t = Validation score   (accuracy)\n",
      "\t3.28s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 257.9s of the 257.9s of remaining time.\n",
      "\t0.6256\t = Validation score   (accuracy)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 256.8s of the 256.8s of remaining time.\n",
      "\t0.624\t = Validation score   (accuracy)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 255.81s of the 255.81s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6305\t = Validation score   (accuracy)\n",
      "\t171.44s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 83.29s of the 83.29s of remaining time.\n",
      "\t0.6158\t = Validation score   (accuracy)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 82.37s of the 82.37s of remaining time.\n",
      "\t0.6059\t = Validation score   (accuracy)\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 81.47s of the 81.47s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6158\t = Validation score   (accuracy)\n",
      "\t2.49s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 77.88s of the 77.88s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.6716\t = Validation score   (accuracy)\n",
      "\t4.71s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 72.06s of the 72.05s of remaining time.\n",
      "\tFitting 6 child models (S1F1 - S1F6) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.5484\t = Validation score   (accuracy)\n",
      "\t8.08s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.94s of the 62.75s of remaining time.\n",
      "\t0.6765\t = Validation score   (accuracy)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 237.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "No improvement since epoch 0: early stopping\n",
      "\t0.45s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t0.9s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t0.61s\t = Training   runtime\n",
      "Fitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t8.17s\t = Training   runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.77s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t0.14s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t2.59s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t3.77s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t0.36s\t = Training   runtime\n",
      "Refit complete, total runtime = 17.78s\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"D:\\ArcgisData\\pred_tl\\pred_moudle\\autogluon_hr\\\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x25673f23340>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取CSV文件使用autogluon训练模型\n",
    "data = pd.read_csv(os.path.join(sample_csv,sample_csv_name))\n",
    "data = data[feature_list+[target_label]]\n",
    "# 列类型转换\n",
    "data['DL'] = data['DL'].astype('str')\n",
    "data['DZ'] = data['DZ'].astype('str')\n",
    "data['SC2'] = data['SC2'].astype('str')\n",
    "# 划分训练集和测试集\n",
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=0)\n",
    "# 使用autogluon训练模型\n",
    "predictor = TabularPredictor(label=target_label,path=modle_save_path,eval_metric='accuracy')\n",
    "predictor.fit(train_data=train_data,time_limit=300,presets='good_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7430894308943089\n",
      "0.6212121212121212\n",
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                           model  score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0            WeightedEnsemble_L2   0.676519       0.141420   11.148052                0.000000           0.358443            2      False         12\n",
      "1          NeuralNetTorch_BAG_L1   0.671593       0.021217    4.711665                0.021217           4.711665            1      False         10\n",
      "2              LightGBMXT_BAG_L1   0.633826       0.047953   27.377481                0.047953          27.377481            1      False          2\n",
      "3                CatBoost_BAG_L1   0.630542       0.019149  171.438420                0.019149         171.438420            1      False          6\n",
      "4        RandomForestGini_BAG_L1   0.625616       0.071203    0.944035                0.071203           0.944035            1       True          4\n",
      "5        RandomForestEntr_BAG_L1   0.623974       0.049721    0.886232                0.049721           0.886232            1       True          5\n",
      "6                 XGBoost_BAG_L1   0.615764       0.029142    2.492022                0.029142           2.492022            1      False          9\n",
      "7          ExtraTreesGini_BAG_L1   0.615764       0.053761    0.804893                0.053761           0.804893            1       True          7\n",
      "8          ExtraTreesEntr_BAG_L1   0.605911       0.053066    0.768163                0.053066           0.768163            1       True          8\n",
      "9                LightGBM_BAG_L1   0.576355       0.042103    3.275087                0.042103           3.275087            1      False          3\n",
      "10          LightGBMLarge_BAG_L1   0.548440       0.136466    8.076295                0.136466           8.076295            1      False         11\n",
      "11        NeuralNetFastAI_BAG_L1   0.530378       0.037995    2.817758                0.037995           2.817758            1      False          1\n",
      "12  RandomForestEntr_BAG_L1_FULL        NaN       0.049721    0.886232                0.049721           0.886232            1       True         17\n",
      "13    ExtraTreesEntr_BAG_L1_FULL        NaN       0.053066    0.768163                0.053066           0.768163            1       True         20\n",
      "14    ExtraTreesGini_BAG_L1_FULL        NaN       0.053761    0.804893                0.053761           0.804893            1       True         19\n",
      "15  RandomForestGini_BAG_L1_FULL        NaN       0.071203    0.944035                0.071203           0.944035            1       True         16\n",
      "16           XGBoost_BAG_L1_FULL        NaN            NaN    0.136385                     NaN           0.136385            1       True         21\n",
      "17      WeightedEnsemble_L2_FULL        NaN            NaN    4.301173                     NaN           0.358443            2       True         24\n",
      "18    NeuralNetTorch_BAG_L1_FULL        NaN            NaN    2.590398                     NaN           2.590398            1       True         22\n",
      "19   NeuralNetFastAI_BAG_L1_FULL        NaN            NaN    0.447783                     NaN           0.447783            1       True         13\n",
      "20          LightGBM_BAG_L1_FULL        NaN            NaN    0.612643                     NaN           0.612643            1       True         15\n",
      "21        LightGBMXT_BAG_L1_FULL        NaN            NaN    0.896609                     NaN           0.896609            1       True         14\n",
      "22     LightGBMLarge_BAG_L1_FULL        NaN            NaN    3.766336                     NaN           3.766336            1       True         23\n",
      "23          CatBoost_BAG_L1_FULL        NaN            NaN    8.167667                     NaN           8.167667            1       True         18\n",
      "Number of models trained: 24\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_XT', 'StackerEnsembleModel_LGB', 'WeightedEnsembleModel', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_CatBoost'}\n",
      "Bagging used: True  (with 6 folds)\n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) : 3 | ['DZ', 'DL', 'SC2']\n",
      "('float', [])    : 4 | ['LAT', 'LON', 'PH', 'SOM']\n",
      "Plot summary of models saved to file: D:\\ArcgisData\\pred_tl\\pred_moudle\\autogluon_hr\\SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'NeuralNetFastAI_BAG_L1': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMXT_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'RandomForestGini_BAG_L1': 'StackerEnsembleModel_RF',\n",
       "  'RandomForestEntr_BAG_L1': 'StackerEnsembleModel_RF',\n",
       "  'CatBoost_BAG_L1': 'StackerEnsembleModel_CatBoost',\n",
       "  'ExtraTreesGini_BAG_L1': 'StackerEnsembleModel_XT',\n",
       "  'ExtraTreesEntr_BAG_L1': 'StackerEnsembleModel_XT',\n",
       "  'XGBoost_BAG_L1': 'StackerEnsembleModel_XGBoost',\n",
       "  'NeuralNetTorch_BAG_L1': 'StackerEnsembleModel_TabularNeuralNetTorch',\n",
       "  'LightGBMLarge_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'NeuralNetFastAI_BAG_L1_FULL': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMXT_BAG_L1_FULL': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1_FULL': 'StackerEnsembleModel_LGB',\n",
       "  'RandomForestGini_BAG_L1_FULL': 'StackerEnsembleModel_RF',\n",
       "  'RandomForestEntr_BAG_L1_FULL': 'StackerEnsembleModel_RF',\n",
       "  'CatBoost_BAG_L1_FULL': 'StackerEnsembleModel_CatBoost',\n",
       "  'ExtraTreesGini_BAG_L1_FULL': 'StackerEnsembleModel_XT',\n",
       "  'ExtraTreesEntr_BAG_L1_FULL': 'StackerEnsembleModel_XT',\n",
       "  'XGBoost_BAG_L1_FULL': 'StackerEnsembleModel_XGBoost',\n",
       "  'NeuralNetTorch_BAG_L1_FULL': 'StackerEnsembleModel_TabularNeuralNetTorch',\n",
       "  'LightGBMLarge_BAG_L1_FULL': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L2_FULL': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'NeuralNetFastAI_BAG_L1': 0.5303776683087028,\n",
       "  'LightGBMXT_BAG_L1': 0.6338259441707718,\n",
       "  'LightGBM_BAG_L1': 0.5763546798029556,\n",
       "  'RandomForestGini_BAG_L1': 0.625615763546798,\n",
       "  'RandomForestEntr_BAG_L1': 0.6239737274220033,\n",
       "  'CatBoost_BAG_L1': 0.6305418719211823,\n",
       "  'ExtraTreesGini_BAG_L1': 0.6157635467980296,\n",
       "  'ExtraTreesEntr_BAG_L1': 0.6059113300492611,\n",
       "  'XGBoost_BAG_L1': 0.6157635467980296,\n",
       "  'NeuralNetTorch_BAG_L1': 0.6715927750410509,\n",
       "  'LightGBMLarge_BAG_L1': 0.548440065681445,\n",
       "  'WeightedEnsemble_L2': 0.6765188834154351,\n",
       "  'NeuralNetFastAI_BAG_L1_FULL': None,\n",
       "  'LightGBMXT_BAG_L1_FULL': None,\n",
       "  'LightGBM_BAG_L1_FULL': None,\n",
       "  'RandomForestGini_BAG_L1_FULL': None,\n",
       "  'RandomForestEntr_BAG_L1_FULL': None,\n",
       "  'CatBoost_BAG_L1_FULL': None,\n",
       "  'ExtraTreesGini_BAG_L1_FULL': None,\n",
       "  'ExtraTreesEntr_BAG_L1_FULL': None,\n",
       "  'XGBoost_BAG_L1_FULL': None,\n",
       "  'NeuralNetTorch_BAG_L1_FULL': None,\n",
       "  'LightGBMLarge_BAG_L1_FULL': None,\n",
       "  'WeightedEnsemble_L2_FULL': None},\n",
       " 'model_best': 'WeightedEnsemble_L2_FULL',\n",
       " 'model_paths': {'NeuralNetFastAI_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\NeuralNetFastAI_BAG_L1\\\\',\n",
       "  'LightGBMXT_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\LightGBMXT_BAG_L1\\\\',\n",
       "  'LightGBM_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\LightGBM_BAG_L1\\\\',\n",
       "  'RandomForestGini_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\RandomForestGini_BAG_L1\\\\',\n",
       "  'RandomForestEntr_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\RandomForestEntr_BAG_L1\\\\',\n",
       "  'CatBoost_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\CatBoost_BAG_L1\\\\',\n",
       "  'ExtraTreesGini_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\ExtraTreesGini_BAG_L1\\\\',\n",
       "  'ExtraTreesEntr_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\ExtraTreesEntr_BAG_L1\\\\',\n",
       "  'XGBoost_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\XGBoost_BAG_L1\\\\',\n",
       "  'NeuralNetTorch_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\NeuralNetTorch_BAG_L1\\\\',\n",
       "  'LightGBMLarge_BAG_L1': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\LightGBMLarge_BAG_L1\\\\',\n",
       "  'WeightedEnsemble_L2': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\WeightedEnsemble_L2\\\\',\n",
       "  'NeuralNetFastAI_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\NeuralNetFastAI_BAG_L1_FULL\\\\',\n",
       "  'LightGBMXT_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\LightGBMXT_BAG_L1_FULL\\\\',\n",
       "  'LightGBM_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\LightGBM_BAG_L1_FULL\\\\',\n",
       "  'RandomForestGini_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\RandomForestGini_BAG_L1_FULL\\\\',\n",
       "  'RandomForestEntr_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\RandomForestEntr_BAG_L1_FULL\\\\',\n",
       "  'CatBoost_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\CatBoost_BAG_L1_FULL\\\\',\n",
       "  'ExtraTreesGini_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\ExtraTreesGini_BAG_L1_FULL\\\\',\n",
       "  'ExtraTreesEntr_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\ExtraTreesEntr_BAG_L1_FULL\\\\',\n",
       "  'XGBoost_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\XGBoost_BAG_L1_FULL\\\\',\n",
       "  'NeuralNetTorch_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\NeuralNetTorch_BAG_L1_FULL\\\\',\n",
       "  'LightGBMLarge_BAG_L1_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\LightGBMLarge_BAG_L1_FULL\\\\',\n",
       "  'WeightedEnsemble_L2_FULL': 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_moudle\\\\autogluon_hr\\\\models\\\\WeightedEnsemble_L2_FULL\\\\'},\n",
       " 'model_fit_times': {'NeuralNetFastAI_BAG_L1': 2.817758083343506,\n",
       "  'LightGBMXT_BAG_L1': 27.37748122215271,\n",
       "  'LightGBM_BAG_L1': 3.2750871181488037,\n",
       "  'RandomForestGini_BAG_L1': 0.9440348148345947,\n",
       "  'RandomForestEntr_BAG_L1': 0.8862318992614746,\n",
       "  'CatBoost_BAG_L1': 171.43842005729675,\n",
       "  'ExtraTreesGini_BAG_L1': 0.8048934936523438,\n",
       "  'ExtraTreesEntr_BAG_L1': 0.7681629657745361,\n",
       "  'XGBoost_BAG_L1': 2.4920222759246826,\n",
       "  'NeuralNetTorch_BAG_L1': 4.711664915084839,\n",
       "  'LightGBMLarge_BAG_L1': 8.076294660568237,\n",
       "  'WeightedEnsemble_L2': 0.3584432601928711,\n",
       "  'NeuralNetFastAI_BAG_L1_FULL': 0.4477832317352295,\n",
       "  'LightGBMXT_BAG_L1_FULL': 0.896608829498291,\n",
       "  'LightGBM_BAG_L1_FULL': 0.612642765045166,\n",
       "  'RandomForestGini_BAG_L1_FULL': 0.9440348148345947,\n",
       "  'RandomForestEntr_BAG_L1_FULL': 0.8862318992614746,\n",
       "  'CatBoost_BAG_L1_FULL': 8.167666673660278,\n",
       "  'ExtraTreesGini_BAG_L1_FULL': 0.8048934936523438,\n",
       "  'ExtraTreesEntr_BAG_L1_FULL': 0.7681629657745361,\n",
       "  'XGBoost_BAG_L1_FULL': 0.13638520240783691,\n",
       "  'NeuralNetTorch_BAG_L1_FULL': 2.5903983116149902,\n",
       "  'LightGBMLarge_BAG_L1_FULL': 3.766336441040039,\n",
       "  'WeightedEnsemble_L2_FULL': 0.3584432601928711},\n",
       " 'model_pred_times': {'NeuralNetFastAI_BAG_L1': 0.0379946231842041,\n",
       "  'LightGBMXT_BAG_L1': 0.04795336723327637,\n",
       "  'LightGBM_BAG_L1': 0.04210305213928223,\n",
       "  'RandomForestGini_BAG_L1': 0.07120347023010254,\n",
       "  'RandomForestEntr_BAG_L1': 0.049721479415893555,\n",
       "  'CatBoost_BAG_L1': 0.019148588180541992,\n",
       "  'ExtraTreesGini_BAG_L1': 0.05376148223876953,\n",
       "  'ExtraTreesEntr_BAG_L1': 0.05306649208068848,\n",
       "  'XGBoost_BAG_L1': 0.029141664505004883,\n",
       "  'NeuralNetTorch_BAG_L1': 0.021216869354248047,\n",
       "  'LightGBMLarge_BAG_L1': 0.13646554946899414,\n",
       "  'WeightedEnsemble_L2': 0.0,\n",
       "  'NeuralNetFastAI_BAG_L1_FULL': None,\n",
       "  'LightGBMXT_BAG_L1_FULL': None,\n",
       "  'LightGBM_BAG_L1_FULL': None,\n",
       "  'RandomForestGini_BAG_L1_FULL': 0.07120347023010254,\n",
       "  'RandomForestEntr_BAG_L1_FULL': 0.049721479415893555,\n",
       "  'CatBoost_BAG_L1_FULL': None,\n",
       "  'ExtraTreesGini_BAG_L1_FULL': 0.05376148223876953,\n",
       "  'ExtraTreesEntr_BAG_L1_FULL': 0.05306649208068848,\n",
       "  'XGBoost_BAG_L1_FULL': None,\n",
       "  'NeuralNetTorch_BAG_L1_FULL': None,\n",
       "  'LightGBMLarge_BAG_L1_FULL': None,\n",
       "  'WeightedEnsemble_L2_FULL': None},\n",
       " 'num_bag_folds': 6,\n",
       " 'max_stack_level': 2,\n",
       " 'num_classes': 32,\n",
       " 'model_hyperparams': {'NeuralNetFastAI_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'LightGBMXT_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'LightGBM_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'RandomForestGini_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'RandomForestEntr_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'CatBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'ExtraTreesGini_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesEntr_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'XGBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'NeuralNetTorch_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'LightGBMLarge_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': False},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetFastAI_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMXT_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestGini_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'RandomForestEntr_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'CatBoost_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'ExtraTreesGini_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesEntr_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'XGBoost_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetTorch_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L1_FULL': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2_FULL': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                            model  score_val  pred_time_val    fit_time  \\\n",
       " 0            WeightedEnsemble_L2   0.676519       0.141420   11.148052   \n",
       " 1          NeuralNetTorch_BAG_L1   0.671593       0.021217    4.711665   \n",
       " 2              LightGBMXT_BAG_L1   0.633826       0.047953   27.377481   \n",
       " 3                CatBoost_BAG_L1   0.630542       0.019149  171.438420   \n",
       " 4        RandomForestGini_BAG_L1   0.625616       0.071203    0.944035   \n",
       " 5        RandomForestEntr_BAG_L1   0.623974       0.049721    0.886232   \n",
       " 6                 XGBoost_BAG_L1   0.615764       0.029142    2.492022   \n",
       " 7          ExtraTreesGini_BAG_L1   0.615764       0.053761    0.804893   \n",
       " 8          ExtraTreesEntr_BAG_L1   0.605911       0.053066    0.768163   \n",
       " 9                LightGBM_BAG_L1   0.576355       0.042103    3.275087   \n",
       " 10          LightGBMLarge_BAG_L1   0.548440       0.136466    8.076295   \n",
       " 11        NeuralNetFastAI_BAG_L1   0.530378       0.037995    2.817758   \n",
       " 12  RandomForestEntr_BAG_L1_FULL        NaN       0.049721    0.886232   \n",
       " 13    ExtraTreesEntr_BAG_L1_FULL        NaN       0.053066    0.768163   \n",
       " 14    ExtraTreesGini_BAG_L1_FULL        NaN       0.053761    0.804893   \n",
       " 15  RandomForestGini_BAG_L1_FULL        NaN       0.071203    0.944035   \n",
       " 16           XGBoost_BAG_L1_FULL        NaN            NaN    0.136385   \n",
       " 17      WeightedEnsemble_L2_FULL        NaN            NaN    4.301173   \n",
       " 18    NeuralNetTorch_BAG_L1_FULL        NaN            NaN    2.590398   \n",
       " 19   NeuralNetFastAI_BAG_L1_FULL        NaN            NaN    0.447783   \n",
       " 20          LightGBM_BAG_L1_FULL        NaN            NaN    0.612643   \n",
       " 21        LightGBMXT_BAG_L1_FULL        NaN            NaN    0.896609   \n",
       " 22     LightGBMLarge_BAG_L1_FULL        NaN            NaN    3.766336   \n",
       " 23          CatBoost_BAG_L1_FULL        NaN            NaN    8.167667   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000000           0.358443            2      False   \n",
       " 1                 0.021217           4.711665            1      False   \n",
       " 2                 0.047953          27.377481            1      False   \n",
       " 3                 0.019149         171.438420            1      False   \n",
       " 4                 0.071203           0.944035            1       True   \n",
       " 5                 0.049721           0.886232            1       True   \n",
       " 6                 0.029142           2.492022            1      False   \n",
       " 7                 0.053761           0.804893            1       True   \n",
       " 8                 0.053066           0.768163            1       True   \n",
       " 9                 0.042103           3.275087            1      False   \n",
       " 10                0.136466           8.076295            1      False   \n",
       " 11                0.037995           2.817758            1      False   \n",
       " 12                0.049721           0.886232            1       True   \n",
       " 13                0.053066           0.768163            1       True   \n",
       " 14                0.053761           0.804893            1       True   \n",
       " 15                0.071203           0.944035            1       True   \n",
       " 16                     NaN           0.136385            1       True   \n",
       " 17                     NaN           0.358443            2       True   \n",
       " 18                     NaN           2.590398            1       True   \n",
       " 19                     NaN           0.447783            1       True   \n",
       " 20                     NaN           0.612643            1       True   \n",
       " 21                     NaN           0.896609            1       True   \n",
       " 22                     NaN           3.766336            1       True   \n",
       " 23                     NaN           8.167667            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          12  \n",
       " 1          10  \n",
       " 2           2  \n",
       " 3           6  \n",
       " 4           4  \n",
       " 5           5  \n",
       " 6           9  \n",
       " 7           7  \n",
       " 8           8  \n",
       " 9           3  \n",
       " 10         11  \n",
       " 11          1  \n",
       " 12         17  \n",
       " 13         20  \n",
       " 14         19  \n",
       " 15         16  \n",
       " 16         21  \n",
       " 17         24  \n",
       " 18         22  \n",
       " 19         13  \n",
       " 20         15  \n",
       " 21         14  \n",
       " 22         23  \n",
       " 23         18  }"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 加载模型\n",
    "predictor = TabularPredictor.load(modle_save_path)\n",
    "# 输出模型再测试集上的准确率,F1值,召回率,精确率\n",
    "print(accuracy_score(train_data[target_label], predictor.predict(train_data.drop(columns=[target_label]))))\n",
    "print(accuracy_score(test_data[target_label], predictor.predict(test_data.drop(columns=[target_label]))))\n",
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2_FULL'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = predictor.get_model_best()\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DZ\n",
      "SOM\n"
     ]
    }
   ],
   "source": [
    "# 试验区域预测数据统一\n",
    "env.workspace = stander_raster_gdb\n",
    "# 查看数据形状的一致性\n",
    "stand_shape = arcpy.RasterToNumPyArray(feature_list[0]).shape\n",
    "err_list = []\n",
    "for one_raster in feature_list[1:]:\n",
    "    if arcpy.RasterToNumPyArray(one_raster).shape != stand_shape:\n",
    "        # 将形状不同的数据输出\n",
    "        print(one_raster)\n",
    "        # 重新设置数据形状\n",
    "        err_list.append(one_raster)\n",
    "# 构造数据\n",
    "feature_array_list = []\n",
    "for one_raster in feature_list:\n",
    "    if one_raster in err_list:\n",
    "        # 跳过形状不同的数据\n",
    "        one_array=resize_arrays(arcpy.RasterToNumPyArray(feature_list[0]),arcpy.RasterToNumPyArray(one_raster), 0)[1]\n",
    "    else:\n",
    "        # 读取数据\n",
    "        one_array = arcpy.RasterToNumPyArray(one_raster)\n",
    "    # 将数据转换为一维数组\n",
    "    one_array = one_array.flatten()\n",
    "    # 将数据添加到列表中\n",
    "    feature_array_list.append(one_array)\n",
    "# 将列表转换为数组\n",
    "feature_array_list = np.column_stack(feature_array_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数组转换为数据帧\n",
    "feature_df = pd.DataFrame(feature_array_list,columns=feature_list)\n",
    "# 修改部分列的数据类型\n",
    "feature_df['DL'] = feature_df['DL'].astype('category')\n",
    "feature_df['DZ'] = feature_df['DZ'].astype('category')\n",
    "feature_df['SC2'] = feature_df['SC2'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,500000,1000000,1500000,2000000,2500000,3000000,3500000,4000000,4500000,5000000,5500000,6000000,6500000,7000000,7500000,8000000,8500000,9000000,9500000,10000000,10500000,11000000,11500000,12000000,12500000,13000000,13500000,14000000,14500000,15000000,15500000,16000000,16500000,17000000,17500000,18000000,18500000,19000000,19500000,20000000,20500000,21000000,21500000,22000000,22500000,23000000,23500000,24000000,24500000,25000000,25500000,26000000,26500000,27000000,27500000,28000000,28500000,29000000,29500000,30000000,30500000,31000000,31500000,32000000,32500000,33000000,33500000,34000000,34500000,35000000,35500000,36000000,36500000,37000000,37500000,38000000,38500000,39000000,39500000,40000000,40500000,41000000,41500000,42000000,42500000,43000000,43500000,44000000,44500000,45000000,45500000,46000000,46500000,47000000,47500000,48000000,48500000,49000000,49500000,50000000,50500000,51000000,51500000,52000000,52500000,53000000,53500000,54000000,54500000,55000000,55500000,56000000,56500000,57000000,57500000,58000000,58500000,59000000,59500000,60000000,60500000,61000000,61500000,62000000,62500000,63000000,63500000,64000000,64500000,65000000,65500000,66000000,66500000,67000000,67500000,68000000,68500000,69000000,69500000,70000000,70500000,71000000,71500000,72000000,72500000,73000000,73500000,74000000,74500000,75000000,75500000,76000000,76500000,77000000,77500000,78000000,78500000,79000000,79500000,80000000,80500000,81000000,81500000,82000000,82500000,83000000,83500000,84000000,84500000,85000000,85500000,86000000,86500000,87000000,87500000,88000000,88500000,89000000,89500000,90000000,90500000,91000000,91500000,92000000,92500000,93000000,93500000,94000000,94500000,95000000,95500000,96000000,96500000,97000000,97500000,98000000,98500000,99000000,99500000,100000000,100500000,101000000,101500000,102000000,102500000,103000000,103500000,104000000,104500000,105000000,105500000,106000000,106500000,107000000,107500000,108000000,108500000,109000000,109500000,110000000,110500000,111000000,111500000,112000000,112500000,113000000,113500000,114000000,114500000,115000000,115500000,116000000,116500000,117000000,117500000,118000000,118500000,119000000,119500000,120000000,120500000,121000000,121500000,122000000,122500000,123000000,123500000,124000000,124500000,125000000,125500000,126000000,126500000,127000000,127500000,128000000,128500000,129000000,129500000,130000000,130500000,131000000,131500000,132000000,132500000,133000000,133500000,134000000,134500000,135000000,135500000,136000000,136500000,137000000,137500000,138000000,138500000,139000000,139500000,140000000,140500000,141000000,141500000,142000000,142500000,143000000,143500000,144000000,144500000,145000000,145500000,146000000,146500000,147000000,147500000,148000000,148500000,149000000,149500000,150000000,150500000,151000000,151500000,152000000,152500000,153000000,153500000,154000000,154500000,155000000,155500000,156000000,156500000,157000000,157500000,158000000,158500000,159000000,159500000,160000000,160500000,161000000,161500000,162000000,162500000,163000000,163500000,164000000,164500000,165000000,"
     ]
    }
   ],
   "source": [
    "# 分割数据并保存为csv文件\n",
    "#清空文件夹内的所有文件\n",
    "try:\n",
    "    shutil.rmtree(cut_csv_path)\n",
    "except:\n",
    "    pass\n",
    "# 创建文件夹\n",
    "try:\n",
    "    os.mkdir(cut_csv_path)\n",
    "except:\n",
    "    pass\n",
    "chunk_size = 500000  # 每个文件的行数\n",
    "total_rows = feature_df.shape[0]\n",
    "for i in range(0, total_rows, chunk_size):\n",
    "    start = i\n",
    "    end = min(i + chunk_size, total_rows)\n",
    "    filename =  os.path.join(cut_csv_path,f'data_chunk_{i}.csv') # 文件名格式可以根据您的需要进行修改\n",
    "    df_chunk = feature_df.iloc[start:end]\n",
    "    df_chunk.to_csv(filename, index=False)\n",
    "    print(i,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_0.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_1000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_1500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_2000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_2500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_3000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_3500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_4000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_4500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_5000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_5500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_6000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_6500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_7000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_7500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_8000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_8500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_9000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_9500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_10000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_10500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_11000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_11500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_12000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_12500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_13000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_13500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_14000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_14500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_15000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_15500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_16000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_16500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_17000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_17500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_18000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_18500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_19000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_19500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_20000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_20500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_21000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_21500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_22000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_22500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_23000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_23500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_24000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_24500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_25000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_25500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_26000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_26500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_27000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_27500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_28000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_28500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_29000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_29500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_30000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_30500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_31000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_31500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_32000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_32500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_33000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_33500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_34000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_34500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_35000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_35500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_36000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_36500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_37000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_37500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_38000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_38500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_39000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_39500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_40000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_40500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_41000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_41500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_42000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_42500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_43000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_43500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_44000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_44500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_45000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_45500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_46000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_46500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_47000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_47500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_48000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_48500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_49000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_49500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_50000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_50500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_51000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_51500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_52000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_52500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_53000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_53500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_54000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_54500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_55000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_55500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_56000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_56500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_57000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_57500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_58000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_58500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_59000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_59500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_60000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_60500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_61000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_61500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_62000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_62500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_63000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_63500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_64000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_64500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_65000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_65500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_66000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_66500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_67000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_67500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_68000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_68500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_69000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_69500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_70000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_70500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_71000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_71500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_72000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_72500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_73000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_73500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_74000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_74500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_75000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_75500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_76000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_76500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_77000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_77500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_78000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_78500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_79000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_79500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_80000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_80500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_81000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_81500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_82000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_82500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_83000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_83500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_84000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_84500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_85000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_85500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_86000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_86500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_87000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_87500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_88000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_88500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_89000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_89500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_90000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_90500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_91000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_91500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_92000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_92500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_93000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_93500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_94000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_94500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_95000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_95500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_96000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_96500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_97000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_97500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_98000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_98500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_99000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_99500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_100000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_100500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_101000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_101500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_102000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_102500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_103000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_103500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_104000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_104500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_105000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_105500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_106000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_106500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_107000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_107500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_108000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_108500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_109000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_109500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_110000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_110500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_111000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_111500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_112000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_112500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_113000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_113500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_114000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_114500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_115000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_115500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_116000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_116500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_117000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_117500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_118000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_118500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_119000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_119500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_120000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_120500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_121000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_121500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_122000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_122500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_123000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_123500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_124000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_124500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_125000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_125500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_126000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_126500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_127000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_127500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_128000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_128500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_129000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_129500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_130000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_130500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_131000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_131500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_132000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_132500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_133000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_133500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_134000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_134500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_135000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_135500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_136000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_136500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_137000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_137500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_138000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_138500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_139000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_139500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_140000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_140500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_141000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_141500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_142000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_142500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_143000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_143500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_144000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_144500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_145000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_145500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_146000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_146500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_147000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_147500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_148000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_148500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_149000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_149500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_150000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_150500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_151000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_151500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_152000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_152500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_153000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_153500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_154000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_154500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_155000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_155500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_156000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_156500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_157000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_157500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_158000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_158500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_159000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_159500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_160000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_160500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_161000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_161500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_162000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_162500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_163000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_163500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_164000000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_164500000.csv',\n",
       " 'D:\\\\ArcgisData\\\\pred_tl\\\\pred_table\\\\autogluon\\\\cut_csv\\\\data_chunk_165000000.csv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "cut_csv_table_list = [os.path.join(cut_csv_path,_) for _ in os.listdir(cut_csv_path)]\n",
    "# 排序\n",
    "sorted_csv_files = sorted(cut_csv_table_list, key=lambda x: int(x.rsplit('_', 1)[-1].split('.')[0]))\n",
    "sorted_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,"
     ]
    }
   ],
   "source": [
    "# 预测试验区域数据\n",
    "#清空文件夹内的所有文件\n",
    "try:\n",
    "    shutil.rmtree(pred_csv_path)\n",
    "except:\n",
    "    pass\n",
    "# 创建文件夹\n",
    "try:\n",
    "    os.mkdir(pred_csv_path)\n",
    "except:\n",
    "    pass\n",
    "pre_n = 0  # 记录预测的表索引\n",
    "for one_table in sorted_csv_files:\n",
    "    data_df = pd.read_csv(one_table)\n",
    "    temp_pred = predictor.predict(data_df)\n",
    "    temp_pred = pd.DataFrame(temp_pred,columns=[target_label])\n",
    "    temp_pred.to_csv(os.path.join(pred_csv_path,f\"{pre_n}.csv\"))\n",
    "    pre_n += 1\n",
    "    print(pre_n,end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取预测结果\n",
    "pre_csv_list = [os.path.join(pred_csv_path,_) for _ in os.listdir(pred_csv_path)]\n",
    "sorted_pre_csv_list = sorted(pre_csv_list,key=lambda x:int(x.rsplit('\\\\', -1)[-1].split('.')[0]))\n",
    "# 保存完整的预测结果\n",
    "pred_df = pd.read_csv(sorted_pre_csv_list[0])\n",
    "for one_pred in sorted_pre_csv_list[1:]:\n",
    "    temp_df = pd.read_csv(one_pred)\n",
    "    pred_df = pd.concat([pred_df,temp_df],axis=0)\n",
    "    print(one_pred,end=',')\n",
    "pred_df.to_csv(os.path.join(merge_csv_path,merge_csv_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv(os.path.join(merge_csv_path,merge_csv_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "pred_df['BM'] = le.fit_transform(pred_df['土种'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['中层厚层泥质黄壤'],\n",
       " 1: ['中层泥质黄壤性土'],\n",
       " 2: ['中泥质黄壤'],\n",
       " 3: ['中泥质黄壤性土'],\n",
       " 4: ['中灰泥质黄壤'],\n",
       " 5: ['中灰泥质黄壤性土'],\n",
       " 6: ['中红泥质黄壤'],\n",
       " 7: ['低泥质黄壤'],\n",
       " 8: ['低泥质黄壤性土'],\n",
       " 9: ['低灰泥质黄壤'],\n",
       " 10: ['低灰泥质黄壤性土'],\n",
       " 11: ['低硅质黄壤'],\n",
       " 12: ['低红泥质黄壤'],\n",
       " 13: ['厚层厚层泥质黄壤'],\n",
       " 14: ['厚层灰泥质黄壤性土'],\n",
       " 15: ['薄层厚层灰泥质黄壤'],\n",
       " 16: ['高泥质黄壤性土'],\n",
       " 17: ['高灰泥质黄壤性土'],\n",
       " 18: ['高红泥质黄壤']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pred_df.groupby('BM')['土种'].apply(lambda x: list(x.unique())).to_dict()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出预测栅格图\n",
    "env.workspace = stander_raster_gdb\n",
    "pred_df['category_encoded'] = pd.factorize(pred_df['BM'])[0]\n",
    "pred_df['category_encoded'] = pred_df['category_encoded'].astype('float32')\n",
    "raster_array = np.reshape(pred_df['category_encoded'].values,arcpy.RasterToNumPyArray(\"DEM\").shape)\n",
    "env.extent = \"DEM\"  # 指定输出栅格的范围\n",
    "pred_result_raster = mask_raster(raster_array, \"DEM\",5)\n",
    "pred_result_raster.save(os.path.join(pred_raster_gdb,f\"{pred_raster_name}_pred\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --->>>概化栅格\n",
    "# 读取栅格 并整型\n",
    "indoor_raster = Raster(os.path.join(r\"D:\\ArcGISProjects\\workspace\\Pred_tl\\Pred_tl.gdb\",\"con_sd_rast1\"))\n",
    "int_raster = Int(indoor_raster)\n",
    "# 1.众数滤波\n",
    "raster_majority_filter = mode_filter(int_raster,\"FOUR\",\"MAJORITY\")\n",
    "\n",
    "# 2.边界清理\n",
    "raster_boundary_clean = clean_boundary(raster_majority_filter,\"NO_SORT\",\"TWO_WAY\")\n",
    "\n",
    "# 3.区域分组\n",
    "raster_region_group = region_group(raster_boundary_clean,\"FOUR\",\"WITHIN\",\"ADD_LINK\",None)\n",
    "\n",
    "# 4.提取区域边界\n",
    "raster_region_boundary = extract_by_attributes(raster_region_group,\"COUNT > 200\")\n",
    "\n",
    "# 5.查找表\n",
    "raster_lookup = lookup(raster_region_group,\"LINK\")\n",
    "\n",
    "# 5.蚕食\n",
    "raster_nibble = nibble(raster_lookup,raster_region_boundary,\"ALL_VALUES\",\"PRESERVE_NODATA\")\n",
    "# 6.区域并保存\n",
    "Int(raster_nibble).save(os.path.join(pred_raster_gdb,f\"RESULT_0821_200\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Con(((\"DL\"==2)|(\"DL\"==5)),100+\"TL_PRED_FZRT_nibble_200\",Con(\"DL\"==1,200+\"TL_PRED_SDT_nibble_200\",300+\"TL_PRED_ZRT_nibble_200\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
